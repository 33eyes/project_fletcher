{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas display settings\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the pickled cleaned encounters-level data\n",
    "df = pd.read_pickle('../data_local/job_posts_parsed_022818.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_descr</th>\n",
       "      <th>company_name</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>email_content</th>\n",
       "      <th>email_date</th>\n",
       "      <th>email_from</th>\n",
       "      <th>email_subject</th>\n",
       "      <th>job_post_link</th>\n",
       "      <th>job_posting</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Geophy</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=1fbba60eb2c9...</td>\n",
       "      <td>GeoPhy is a company that offers independent pr...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloomberg is a company dedicated to helping so...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=a1e3470c4783...</td>\n",
       "      <td>Job Requisition Number:65487\\n\\nAt Bloomberg's...</td>\n",
       "      <td>Machine Learning Educator / Research Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Simons Foundation</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=d4a1c8ed325a...</td>\n",
       "      <td>The Simons Foundation is beginning a new compu...</td>\n",
       "      <td>Systems Software Engineer - Neuroscience</td>\n",
       "      <td>New York, NY 10010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just because you match a job on paper, doesn't...</td>\n",
       "      <td>KellyMitchell</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=4dbf56a8c979...</td>\n",
       "      <td>KellyMitchell matches the best IT and business...</td>\n",
       "      <td>Data Scientist - Intermediate</td>\n",
       "      <td>Chesterfield, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Headquartered in Woonsocket, Rhode Island, CVS...</td>\n",
       "      <td>CVS Health</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=83793cb590f2...</td>\n",
       "      <td>The Systems Data Analyst is responsible for pe...</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       company_descr       company_name  \\\n",
       "0                                                                Geophy   \n",
       "1  Bloomberg is a company dedicated to helping so...          Bloomberg   \n",
       "2                                                     Simons Foundation   \n",
       "3  Just because you match a job on paper, doesn't...      KellyMitchell   \n",
       "4  Headquartered in Woonsocket, Rhode Island, CVS...         CVS Health   \n",
       "\n",
       "  date_posted                                      email_content  \\\n",
       "0  3 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "1  4 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "2  5 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "3  4 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "4  5 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "\n",
       "                              email_date        email_from  \\\n",
       "0  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "1  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "2  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "3  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "4  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "\n",
       "                                       email_subject  \\\n",
       "0  You have new recommended jobs: Data Scientist ...   \n",
       "1  You have new recommended jobs: Data Scientist ...   \n",
       "2  You have new recommended jobs: Data Scientist ...   \n",
       "3  You have new recommended jobs: Data Scientist ...   \n",
       "4  You have new recommended jobs: Data Scientist ...   \n",
       "\n",
       "                                       job_post_link  \\\n",
       "0  https://www.indeed.com/viewjob?jk=1fbba60eb2c9...   \n",
       "1  https://www.indeed.com/viewjob?jk=a1e3470c4783...   \n",
       "2  https://www.indeed.com/viewjob?jk=d4a1c8ed325a...   \n",
       "3  https://www.indeed.com/viewjob?jk=4dbf56a8c979...   \n",
       "4  https://www.indeed.com/viewjob?jk=83793cb590f2...   \n",
       "\n",
       "                                         job_posting  \\\n",
       "0  GeoPhy is a company that offers independent pr...   \n",
       "1  Job Requisition Number:65487\\n\\nAt Bloomberg's...   \n",
       "2  The Simons Foundation is beginning a new compu...   \n",
       "3  KellyMitchell matches the best IT and business...   \n",
       "4  The Systems Data Analyst is responsible for pe...   \n",
       "\n",
       "                                       job_title            location  \n",
       "0                                 Data Scientist        New York, NY  \n",
       "1  Machine Learning Educator / Research Engineer        New York, NY  \n",
       "2       Systems Software Engineer - Neuroscience  New York, NY 10010  \n",
       "3                  Data Scientist - Intermediate    Chesterfield, MO  \n",
       "4                                   Data Analyst        New York, NY  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_descr</th>\n",
       "      <th>company_name</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>email_content</th>\n",
       "      <th>email_date</th>\n",
       "      <th>email_from</th>\n",
       "      <th>email_subject</th>\n",
       "      <th>job_post_link</th>\n",
       "      <th>job_posting</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>63</td>\n",
       "      <td>108</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>127</td>\n",
       "      <td>114</td>\n",
       "      <td>85</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>Indeed Job Alert data science jobs  Jobs 1-30 ...</td>\n",
       "      <td>Tue, 27 Feb 2018 18:01:23 -0600 (CST)</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>30+ new data science jobs</td>\n",
       "      <td>http://talentsolvers.com/career/?cjobid=KM3317...</td>\n",
       "      <td></td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       company_descr company_name date_posted  \\\n",
       "count            128          128         128   \n",
       "unique            63          108          24   \n",
       "top                                 1 day ago   \n",
       "freq              40            3          43   \n",
       "\n",
       "                                            email_content  \\\n",
       "count                                                 128   \n",
       "unique                                                  6   \n",
       "top     Indeed Job Alert data science jobs  Jobs 1-30 ...   \n",
       "freq                                                   33   \n",
       "\n",
       "                                   email_date email_from  \\\n",
       "count                                     128        128   \n",
       "unique                                      6          3   \n",
       "top     Tue, 27 Feb 2018 18:01:23 -0600 (CST)     Indeed   \n",
       "freq                                       33         66   \n",
       "\n",
       "                    email_subject  \\\n",
       "count                         128   \n",
       "unique                          4   \n",
       "top     30+ new data science jobs   \n",
       "freq                           66   \n",
       "\n",
       "                                            job_post_link job_posting  \\\n",
       "count                                                 128         128   \n",
       "unique                                                127         114   \n",
       "top     http://talentsolvers.com/career/?cjobid=KM3317...               \n",
       "freq                                                    2           3   \n",
       "\n",
       "             job_title      location  \n",
       "count              128           128  \n",
       "unique              85            82  \n",
       "top     Data Scientist  New York, NY  \n",
       "freq                31            20  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 128 entries, 0 to 127\n",
      "Data columns (total 11 columns):\n",
      "company_descr    128 non-null object\n",
      "company_name     128 non-null object\n",
      "date_posted      128 non-null object\n",
      "email_content    128 non-null object\n",
      "email_date       128 non-null object\n",
      "email_from       128 non-null object\n",
      "email_subject    128 non-null object\n",
      "job_post_link    128 non-null object\n",
      "job_posting      128 non-null object\n",
      "job_title        128 non-null object\n",
      "location         128 non-null object\n",
      "dtypes: object(11)\n",
      "memory usage: 11.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                        3\n",
       "McKinsey & Company                      3\n",
       "Taboola                                 3\n",
       "IBM                                     3\n",
       "Rockstar New York                       2\n",
       "Glocomms                                2\n",
       "BDIPlus                                 2\n",
       "PerkinElmer                             2\n",
       "Geophy                                  2\n",
       "Western & Southern Financial Group      2\n",
       "GSSA, LLC                               2\n",
       "Verisk Maplecroft                       2\n",
       "Simons Foundation                       2\n",
       "Arthur Grand Technologies Inc           2\n",
       "Bristol-Myers Squibb                    2\n",
       "OppenheimerFunds                        2\n",
       "Comcast                                 1\n",
       "Redolent, Inc.                          1\n",
       "Tax                                     1\n",
       "Clearlink                               1\n",
       "Integral Ad Science                     1\n",
       "FIR.ai                                  1\n",
       "ZyLAB                                   1\n",
       "Nite Services Recruitment               1\n",
       "NOKIA                                   1\n",
       "                                       ..\n",
       "Syneos Health Commerical Solutions      1\n",
       "Nielsen                                 1\n",
       "Camber Corporation                      1\n",
       "Broward College                         1\n",
       "Swagelok                                1\n",
       "Katerra Inc                             1\n",
       "Baystate Health                         1\n",
       "Microsoft                               1\n",
       "Amazon.com                              1\n",
       "GroupM                                  1\n",
       "Two Sigma                               1\n",
       "Capgemini                               1\n",
       "Minnesota IT Services                   1\n",
       "DuPont                                  1\n",
       "National Renewable Energy Laboratory    1\n",
       "Axelerate                               1\n",
       "Lazard Ltd.                             1\n",
       "Software Technology Inc                 1\n",
       "Tapestry                                1\n",
       "SciTec                                  1\n",
       "KellyMitchell                           1\n",
       "Apex                                    1\n",
       "Facebook                                1\n",
       "Progressive Leasing                     1\n",
       "SAIC                                    1\n",
       "Name: company_name, Length: 108, dtype: int64"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.company_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Director, Data Science & Platforms - SMB           5\n",
       "Data Scientist, Analytics (Instagram)              3\n",
       "Software Engineer, Machine Learning                2\n",
       "Data Scientist, SMB                                2\n",
       "Data Science Manager, Analytics (Instagram NYC)    1\n",
       "Name: job_title, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.company_name == 'Facebook'].job_title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data Scientist                                                          31\n",
       "                                                                         3\n",
       "Data Scientist - Healthcare Analytics & Delivery, Advanced Analytics     3\n",
       "Data Science Intern                                                      3\n",
       "Data Science Associate                                                   2\n",
       "Data Science Analytics Leader                                            2\n",
       "Data Science Statistician                                                2\n",
       "Data Science Analyst                                                     2\n",
       "Data Analyst                                                             2\n",
       "Systems Software Engineer - Neuroscience                                 2\n",
       "Senior Data Engineer (Data Science Team)                                 2\n",
       "Senior Associate, Data Science                                           1\n",
       "Manager, GPS Analytics and Data Science                                  1\n",
       "Data Science Manager                                                     1\n",
       "Big Data and Data Science Consultant                                     1\n",
       "Senior Data Scientist                                                    1\n",
       "Director (Data Science & Machine Learning)                               1\n",
       "Data Analyst - Data Science                                              1\n",
       "Scientist 4, Entrprs Data & Anlytcs                                      1\n",
       "IBM Data Scientist: Apprentice                                           1\n",
       "Analyst, Data Science and Analytics                                      1\n",
       "Data Scientist - Intermediate                                            1\n",
       "Junior Imaging Sensor Data Scientist                                     1\n",
       "Data Clinic - Part-time Internship Fall 2018 (Data Science for Good)     1\n",
       "Data Science Internship                                                  1\n",
       "                                                                        ..\n",
       "Data Science for SU                                                      1\n",
       "Data Scientist 3                                                         1\n",
       "Lead Data Scientist                                                      1\n",
       "Analyst, Data Science                                                    1\n",
       "Sr. Data Scientist                                                       1\n",
       "Material Analyst - Pre-Review                                            1\n",
       "Summer 2018 Intern- Data Science                                         1\n",
       "Software Engineer, Machine Learning                                      1\n",
       "Researcher III – Data Analytics                                          1\n",
       "Technical Project Manager                                                1\n",
       "Senior Data Scientist (Open to Remote)                                   1\n",
       "Data Scientist - Jr. / Mid Level                                         1\n",
       "Data Science Research Summer Coop                                        1\n",
       "Data Scientists - Prediction Modeling                                    1\n",
       "Sirius Healthcare, Chief Data Officer                                    1\n",
       "Data Science Programmer                                                  1\n",
       "Data Science Engineer                                                    1\n",
       "Business Analyst/Data Analyst/Data Scientist                             1\n",
       "District Director, Research and Data Science                             1\n",
       "Clinical Data Analyst- Quantitative Data Science                         1\n",
       "Data Scientist 4                                                         1\n",
       "Principal Consultant - Data & Analytics                                  1\n",
       "Data Science Analyst Intern                                              1\n",
       "Data Scientist Job                                                       1\n",
       "Data Science Analyst Internship                                          1\n",
       "Name: job_title, Length: 85, dtype: int64"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.job_title.value_counts()\n",
    "# Standardize this \n",
    "# Maybe derive role and level from this\n",
    "# levels = [intern, junior, senior]\n",
    "# roles = [scientist, engineer, analyst, manager, director, consultant, other]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2['job_role'] = ''\n",
    "df2['job_level'] = ''\n",
    "#df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# roles\n",
    "scientist_roles = ['scientist', 'statistician', 'data science specialist']\n",
    "engineer_roles = ['engineer']\n",
    "analyst_roles = ['analyst']\n",
    "director_roles = ['director', 'vice president', 'vp', 'svp', 'cto', 'cte']\n",
    "manager_roles = ['manager', 'leader', 'lead']\n",
    "developer_roles = ['developer', 'programmer', 'architect']\n",
    "consultant_roles = ['consultant']\n",
    "researcher_roles = ['researcher']\n",
    "academic_roles = ['tenure', 'post doc']\n",
    "other_roles = ['data science', 'big data', 'machine learning']\n",
    "\n",
    "\n",
    "# levels = [intern, junior, senior]\n",
    "intern_levels = ['intern']\n",
    "junior_levels = ['junior', 'jr', 'associate']\n",
    "senior_levels = ['senior', 'sr', 'principal'] + director_roles + manager_roles\n",
    "\n",
    "for i in range(len(df2)):\n",
    "     \n",
    "    if not df2['job_role'][i]:\n",
    "        for role in academic_roles:\n",
    "            if role in df2.job_title[i].lower():\n",
    "                df2['job_role'][i] = 'data science academia' \n",
    "    if not df2['job_role'][i]:            \n",
    "        for role in scientist_roles:\n",
    "            if role in df2.job_title[i].lower():\n",
    "                df2['job_role'][i] = 'data scientist' \n",
    "    if not df2['job_role'][i]:\n",
    "        for role in engineer_roles:\n",
    "            if role in df2.job_title[i].lower():\n",
    "                df2['job_role'][i] = 'data engineer' \n",
    "    if not df2['job_role'][i]:\n",
    "        for role in analyst_roles:\n",
    "            if role in df2.job_title[i].lower():\n",
    "                df2['job_role'][i] = 'data analyst' \n",
    "    if not df2['job_role'][i]:\n",
    "        for role in director_roles:\n",
    "            if role in df2.job_title[i].lower():\n",
    "                df2['job_role'][i] = 'data science director' \n",
    "    if not df2['job_role'][i]:\n",
    "        for role in manager_roles:\n",
    "            if role in df2.job_title[i].lower():\n",
    "                df2['job_role'][i] = 'data science manager' \n",
    "    if not df2['job_role'][i]:\n",
    "        for role in developer_roles:\n",
    "            if role in df2.job_title[i].lower():\n",
    "                df2['job_role'][i] = 'data science developer' \n",
    "    if not df2['job_role'][i]:\n",
    "        for role in consultant_roles:\n",
    "            if role in df2.job_title[i].lower():\n",
    "                df2['job_role'][i] = 'data science consultant'\n",
    "    if not df2['job_role'][i]:\n",
    "        for role in researcher_roles:\n",
    "            if role in df2.job_title[i].lower():\n",
    "                df2['job_role'][i] = 'data science researcher' \n",
    "    if not df2['job_role'][i]:\n",
    "        for role in other_roles:\n",
    "            if role in df2.job_title[i].lower():\n",
    "                df2['job_role'][i] = 'data science other' \n",
    "                \n",
    "    \n",
    "    \n",
    "    for level in intern_levels:\n",
    "        if level in df2.job_title[i].lower():\n",
    "            df2['job_level'][i] = 'intern'\n",
    "    if not df2['job_level'][i]:\n",
    "        for level in junior_levels:\n",
    "            if level in df2.job_title[i].lower():\n",
    "                df2['job_level'][i] = 'junior'\n",
    "    if not df2['job_level'][i]:\n",
    "        for level in senior_levels:\n",
    "            if level in df2.job_title[i].lower():\n",
    "                df2['job_level'][i] = 'senior'\n",
    "        \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          11\n",
       "intern     2\n",
       "Name: job_level, dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2.job_role == 'data analyst'].job_level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data scientist             65\n",
       "data science other         17\n",
       "data analyst               13\n",
       "data science manager        9\n",
       "data engineer               9\n",
       "data science director       5\n",
       "                            5\n",
       "data science developer      2\n",
       "data science consultant     2\n",
       "data science researcher     1\n",
       "Name: job_role, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.job_role.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          83\n",
       "senior    25\n",
       "intern    12\n",
       "junior     8\n",
       "Name: job_level, dtype: int64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.job_level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_descr</th>\n",
       "      <th>company_name</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>email_content</th>\n",
       "      <th>email_date</th>\n",
       "      <th>email_from</th>\n",
       "      <th>email_subject</th>\n",
       "      <th>job_post_link</th>\n",
       "      <th>job_posting</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>job_role</th>\n",
       "      <th>job_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Geophy</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=1fbba60eb2c9...</td>\n",
       "      <td>GeoPhy is a company that offers independent pr...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>data scientist</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloomberg is a company dedicated to helping so...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=a1e3470c4783...</td>\n",
       "      <td>Job Requisition Number:65487\\n\\nAt Bloomberg's...</td>\n",
       "      <td>Machine Learning Educator / Research Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>data engineer</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Simons Foundation</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=d4a1c8ed325a...</td>\n",
       "      <td>The Simons Foundation is beginning a new compu...</td>\n",
       "      <td>Systems Software Engineer - Neuroscience</td>\n",
       "      <td>New York, NY 10010</td>\n",
       "      <td>data engineer</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just because you match a job on paper, doesn't...</td>\n",
       "      <td>KellyMitchell</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=4dbf56a8c979...</td>\n",
       "      <td>KellyMitchell matches the best IT and business...</td>\n",
       "      <td>Data Scientist - Intermediate</td>\n",
       "      <td>Chesterfield, MO</td>\n",
       "      <td>data scientist</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Headquartered in Woonsocket, Rhode Island, CVS...</td>\n",
       "      <td>CVS Health</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=83793cb590f2...</td>\n",
       "      <td>The Systems Data Analyst is responsible for pe...</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>data analyst</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       company_descr       company_name  \\\n",
       "0                                                                Geophy   \n",
       "1  Bloomberg is a company dedicated to helping so...          Bloomberg   \n",
       "2                                                     Simons Foundation   \n",
       "3  Just because you match a job on paper, doesn't...      KellyMitchell   \n",
       "4  Headquartered in Woonsocket, Rhode Island, CVS...         CVS Health   \n",
       "\n",
       "  date_posted                                      email_content  \\\n",
       "0  3 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "1  4 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "2  5 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "3  4 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "4  5 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "\n",
       "                              email_date        email_from  \\\n",
       "0  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "1  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "2  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "3  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "4  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "\n",
       "                                       email_subject  \\\n",
       "0  You have new recommended jobs: Data Scientist ...   \n",
       "1  You have new recommended jobs: Data Scientist ...   \n",
       "2  You have new recommended jobs: Data Scientist ...   \n",
       "3  You have new recommended jobs: Data Scientist ...   \n",
       "4  You have new recommended jobs: Data Scientist ...   \n",
       "\n",
       "                                       job_post_link  \\\n",
       "0  https://www.indeed.com/viewjob?jk=1fbba60eb2c9...   \n",
       "1  https://www.indeed.com/viewjob?jk=a1e3470c4783...   \n",
       "2  https://www.indeed.com/viewjob?jk=d4a1c8ed325a...   \n",
       "3  https://www.indeed.com/viewjob?jk=4dbf56a8c979...   \n",
       "4  https://www.indeed.com/viewjob?jk=83793cb590f2...   \n",
       "\n",
       "                                         job_posting  \\\n",
       "0  GeoPhy is a company that offers independent pr...   \n",
       "1  Job Requisition Number:65487\\n\\nAt Bloomberg's...   \n",
       "2  The Simons Foundation is beginning a new compu...   \n",
       "3  KellyMitchell matches the best IT and business...   \n",
       "4  The Systems Data Analyst is responsible for pe...   \n",
       "\n",
       "                                       job_title            location  \\\n",
       "0                                 Data Scientist        New York, NY   \n",
       "1  Machine Learning Educator / Research Engineer        New York, NY   \n",
       "2       Systems Software Engineer - Neuroscience  New York, NY 10010   \n",
       "3                  Data Scientist - Intermediate    Chesterfield, MO   \n",
       "4                                   Data Analyst        New York, NY   \n",
       "\n",
       "         job_role job_level  \n",
       "0  data scientist            \n",
       "1   data engineer            \n",
       "2   data engineer            \n",
       "3  data scientist            \n",
       "4    data analyst            "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "New York, NY                  20\n",
       "New York City, NY, US          5\n",
       "New York, NY 10022             4\n",
       "Los Angeles, CA                4\n",
       "New York, New York             3\n",
       "San Francisco, CA              3\n",
       "Research Triangle Park, NC     3\n",
       "                               3\n",
       "Cincinnati, OH                 2\n",
       "Herndon, VA 20170              2\n",
       "Chicago, IL                    2\n",
       "Denver, CO                     2\n",
       "Waltham, MA                    2\n",
       "Columbus, OH                   2\n",
       "Sunnyvale, CA                  2\n",
       "New York, NY 10010             2\n",
       "Cary, NC                       2\n",
       "Israel                         1\n",
       "Oakbrook Terrace, IL, US       1\n",
       "Hopewell, NJ, US               1\n",
       "Dearborn, MI 48126             1\n",
       "Salt Lake County, UT, US       1\n",
       "Freeport, TX 77541             1\n",
       "New York, NY 10003             1\n",
       "Brooklyn, NY, US               1\n",
       "                              ..\n",
       "White Plains, NY 10601         1\n",
       "Greater New York City Area     1\n",
       "Mahwah, New Jersey             1\n",
       "Bellevue, WA 98005             1\n",
       "United States                  1\n",
       "Murray Hill, NJ                1\n",
       "Houston, TX 77042              1\n",
       "New Brunswick, NJ              1\n",
       "Davie, FL                      1\n",
       "Westborough, MA 01581          1\n",
       "Huntsville, AL                 1\n",
       "San Diego, CA                  1\n",
       "Jersey City, NJ                1\n",
       "Redmond, WA                    1\n",
       "Plymouth Meeting, PA 19462     1\n",
       "Boyers, PA                     1\n",
       "Philadelphia, PA 19127         1\n",
       "Wilmington, DE                 1\n",
       "Golden, CO                     1\n",
       "Richardson, TX                 1\n",
       "Draper, UT                     1\n",
       "Dearborn, MI 48121             1\n",
       "Grand Marais, MN 55604         1\n",
       "Greensboro, NC                 1\n",
       "Sunnyvale, CA 94086            1\n",
       "Name: location, Length: 82, dtype: int64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.location.value_counts()\n",
    "# Derive country, state andd city into separate fields from this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive country, state andd city into separate fields\n",
    "df2['country'] = ''\n",
    "df2['state'] = ''\n",
    "df2['city'] = ''\n",
    "\n",
    "murica_list = [', US', 'USA', 'United States', 'America']\n",
    "\n",
    "\n",
    "for i in range(len(df2)):\n",
    "    location = df2.location[i]\n",
    "    location = re.sub(r'\\d', r'', location)\n",
    "    location = re.sub(r'(Greater )?New York City( Area)?', r'New York', location, flags=re.IGNORECASE)\n",
    "    location = re.sub(r'San Francisco Bay Area', r'San Francisco', location, flags=re.IGNORECASE)\n",
    "    location = re.sub(r'Pennsylvania', r'PA', location, flags=re.IGNORECASE)\n",
    "    location = re.sub(r'Metro Area', r'', location, flags=re.IGNORECASE)\n",
    "    location = location.strip()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # USA\n",
    "    #linky_clean = re.search(r'(?<=\\() ?https?://.*(?=\\))', linky, re.IGNORECASE).group().strip()\n",
    "    for mur in murica_list:\n",
    "        if mur.lower() in location.lower():\n",
    "            df2['country'][i] = 'USA'\n",
    "    \n",
    "    ## International locations\n",
    "    # Delhi, IN\n",
    "    if location.lower() == 'delhi, in':\n",
    "        df2['city'][i] = 'Delhi' \n",
    "        df2['country'][i] = 'India'\n",
    "    # Bangalore, IN\n",
    "    elif location.lower() == 'bangalore, in':\n",
    "        df2['city'][i] = 'Bangalore' \n",
    "        df2['country'][i] = 'India'\n",
    "    # San José, CR\n",
    "    elif location.lower() == 'san josé, cr':\n",
    "        df2['city'][i] = 'San José' \n",
    "        df2['country'][i] = 'Costa Rica'\n",
    "    # São Paulo Area, Brazil\n",
    "    elif location.lower() == 'são paulo area, brazil':\n",
    "        df2['city'][i] = 'São Paulo' \n",
    "        df2['country'][i] = 'Brazil'\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # US city and state\n",
    "        if location.lower() == 'washington d.c.':\n",
    "            df2['city'][i] = 'Washington'\n",
    "            df2['state'][i] = 'DC'\n",
    "            df2['country'][i] = 'USA'\n",
    "        elif location.lower() == 'new york':\n",
    "            df2['city'][i] = 'New York'\n",
    "            df2['state'][i] = 'NY'\n",
    "            df2['country'][i] = 'USA'\n",
    "        elif location.lower() == 'michigan':\n",
    "            df2['state'][i] = 'MI'\n",
    "            df2['country'][i] = 'USA'\n",
    "        elif location.lower() == 'minnesota':\n",
    "            df2['state'][i] = 'MN'\n",
    "            df2['country'][i] = 'USA'\n",
    "        elif location.lower() == 'california':\n",
    "            df2['state'][i] = 'CA'\n",
    "            df2['country'][i] = 'USA'\n",
    "        elif location.lower() == 'maryland':\n",
    "            df2['state'][i] = 'MD'\n",
    "            df2['country'][i] = 'USA'\n",
    "        else:    \n",
    "            location = re.sub(r', US', r'', location, flags=re.IGNORECASE)\n",
    "            if ',' in location:\n",
    "                df2['city'][i], df2['state'][i] = location.split(',')\n",
    "                df2['country'][i] = 'USA'\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA      122\n",
       "           5\n",
       "India      1\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 3\n",
       "Israel           1\n",
       "United States    1\n",
       "Virginia         1\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2.city == ''].location.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdelta = timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2018, 2, 26)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(datetime.today() - tdelta).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1 day ago              43\n",
       "2 days ago             14\n",
       "4 days ago             11\n",
       "Posted 1 day ago        9\n",
       "5 days ago              8\n",
       "Posted 2 days ago       7\n",
       "3 days ago              6\n",
       "22 hours ago            4\n",
       "6 days ago              4\n",
       "11 days ago             3\n",
       "                        3\n",
       "Posted 21 hours ago     2\n",
       "7 days ago              2\n",
       "18 days ago             2\n",
       "30+ days ago            1\n",
       "Posted 20 hours ago     1\n",
       "9 days ago              1\n",
       "15 days ago             1\n",
       "Posted 18 hours ago     1\n",
       "23 hours ago            1\n",
       "Posted 9 hours ago      1\n",
       "Posted 23 hours ago     1\n",
       "21 hours ago            1\n",
       "21 days ago             1\n",
       "Name: date_posted, dtype: int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.date_posted.value_counts()\n",
    "# Calculate the actual dates from this - to be run on the day of scraping (02/26/18 for this run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7 days ago'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df2.date_posted[0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7 day'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_posted_str = re.sub(r'[\\+s]', r'', test)\n",
    "#date_posted_str\n",
    "ndays_text = re.search(r'\\d* ((hour)|(day)|(week))', date_posted_str, re.IGNORECASE).group().strip()\n",
    "ndays_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date_posted(date_posted_str, refdt=datetime.today()):\n",
    "    # ndays = something from date_posted_str\n",
    "    date_posted_str = re.sub(r'[\\+s]', r'', date_posted_str)\n",
    "    if re.search(r'\\d* ((hour)|(day)|(week))', date_posted_str, re.IGNORECASE):\n",
    "        ndays_text = re.search(r'\\d* ((hour)|(day)|(week))', date_posted_str, re.IGNORECASE).group().strip()\n",
    "        n, period = ndays_text.split(' ')\n",
    "        ndays = int(n)\n",
    "        if period == 'hour':\n",
    "            ndays = 0\n",
    "        elif period == 'week':\n",
    "            ndays = ndays * 7\n",
    "    \n",
    "        tdelta = timedelta(days=ndays)\n",
    "        return (refdt - tdelta).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['job_post_date'] = df2['date_posted'].apply(clean_date_posted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-02-27    52\n",
       "2018-02-26    21\n",
       "2018-02-28    12\n",
       "2018-02-24    11\n",
       "2018-02-23     8\n",
       "2018-02-25     6\n",
       "2018-02-22     4\n",
       "2018-02-17     3\n",
       "2018-02-21     2\n",
       "2018-02-10     2\n",
       "2018-02-19     1\n",
       "2018-02-07     1\n",
       "2018-01-29     1\n",
       "2018-02-13     1\n",
       "Name: job_post_date, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.job_post_date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_descr</th>\n",
       "      <th>company_name</th>\n",
       "      <th>date_posted</th>\n",
       "      <th>email_content</th>\n",
       "      <th>email_date</th>\n",
       "      <th>email_from</th>\n",
       "      <th>email_subject</th>\n",
       "      <th>job_post_link</th>\n",
       "      <th>job_posting</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>job_role</th>\n",
       "      <th>job_level</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>job_post_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Geophy</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=1fbba60eb2c9...</td>\n",
       "      <td>GeoPhy is a company that offers independent pr...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>data scientist</td>\n",
       "      <td></td>\n",
       "      <td>USA</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>2018-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bloomberg is a company dedicated to helping so...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=a1e3470c4783...</td>\n",
       "      <td>Job Requisition Number:65487\\n\\nAt Bloomberg's...</td>\n",
       "      <td>Machine Learning Educator / Research Engineer</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>data engineer</td>\n",
       "      <td></td>\n",
       "      <td>USA</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>2018-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Simons Foundation</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=d4a1c8ed325a...</td>\n",
       "      <td>The Simons Foundation is beginning a new compu...</td>\n",
       "      <td>Systems Software Engineer - Neuroscience</td>\n",
       "      <td>New York, NY 10010</td>\n",
       "      <td>data engineer</td>\n",
       "      <td></td>\n",
       "      <td>USA</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>2018-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just because you match a job on paper, doesn't...</td>\n",
       "      <td>KellyMitchell</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=4dbf56a8c979...</td>\n",
       "      <td>KellyMitchell matches the best IT and business...</td>\n",
       "      <td>Data Scientist - Intermediate</td>\n",
       "      <td>Chesterfield, MO</td>\n",
       "      <td>data scientist</td>\n",
       "      <td></td>\n",
       "      <td>USA</td>\n",
       "      <td>MO</td>\n",
       "      <td>Chesterfield</td>\n",
       "      <td>2018-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Headquartered in Woonsocket, Rhode Island, CVS...</td>\n",
       "      <td>CVS Health</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Recommended Jobs for You  Jobs 1 to 20 of 20 r...</td>\n",
       "      <td>Mon, 26 Feb 2018 13:47:08 -0600 (CST)</td>\n",
       "      <td>Indeed Job Alert</td>\n",
       "      <td>You have new recommended jobs: Data Scientist ...</td>\n",
       "      <td>https://www.indeed.com/viewjob?jk=83793cb590f2...</td>\n",
       "      <td>The Systems Data Analyst is responsible for pe...</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>data analyst</td>\n",
       "      <td></td>\n",
       "      <td>USA</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>2018-02-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       company_descr       company_name  \\\n",
       "0                                                                Geophy   \n",
       "1  Bloomberg is a company dedicated to helping so...          Bloomberg   \n",
       "2                                                     Simons Foundation   \n",
       "3  Just because you match a job on paper, doesn't...      KellyMitchell   \n",
       "4  Headquartered in Woonsocket, Rhode Island, CVS...         CVS Health   \n",
       "\n",
       "  date_posted                                      email_content  \\\n",
       "0  3 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "1  4 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "2  5 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "3  4 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "4  5 days ago  Recommended Jobs for You  Jobs 1 to 20 of 20 r...   \n",
       "\n",
       "                              email_date        email_from  \\\n",
       "0  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "1  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "2  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "3  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "4  Mon, 26 Feb 2018 13:47:08 -0600 (CST)  Indeed Job Alert   \n",
       "\n",
       "                                       email_subject  \\\n",
       "0  You have new recommended jobs: Data Scientist ...   \n",
       "1  You have new recommended jobs: Data Scientist ...   \n",
       "2  You have new recommended jobs: Data Scientist ...   \n",
       "3  You have new recommended jobs: Data Scientist ...   \n",
       "4  You have new recommended jobs: Data Scientist ...   \n",
       "\n",
       "                                       job_post_link  \\\n",
       "0  https://www.indeed.com/viewjob?jk=1fbba60eb2c9...   \n",
       "1  https://www.indeed.com/viewjob?jk=a1e3470c4783...   \n",
       "2  https://www.indeed.com/viewjob?jk=d4a1c8ed325a...   \n",
       "3  https://www.indeed.com/viewjob?jk=4dbf56a8c979...   \n",
       "4  https://www.indeed.com/viewjob?jk=83793cb590f2...   \n",
       "\n",
       "                                         job_posting  \\\n",
       "0  GeoPhy is a company that offers independent pr...   \n",
       "1  Job Requisition Number:65487\\n\\nAt Bloomberg's...   \n",
       "2  The Simons Foundation is beginning a new compu...   \n",
       "3  KellyMitchell matches the best IT and business...   \n",
       "4  The Systems Data Analyst is responsible for pe...   \n",
       "\n",
       "                                       job_title            location  \\\n",
       "0                                 Data Scientist        New York, NY   \n",
       "1  Machine Learning Educator / Research Engineer        New York, NY   \n",
       "2       Systems Software Engineer - Neuroscience  New York, NY 10010   \n",
       "3                  Data Scientist - Intermediate    Chesterfield, MO   \n",
       "4                                   Data Analyst        New York, NY   \n",
       "\n",
       "         job_role job_level country state          city job_post_date  \n",
       "0  data scientist               USA    NY      New York    2018-02-25  \n",
       "1   data engineer               USA    NY      New York    2018-02-24  \n",
       "2   data engineer               USA    NY      New York    2018-02-23  \n",
       "3  data scientist               USA    MO  Chesterfield    2018-02-24  \n",
       "4    data analyst               USA    NY      New York    2018-02-23  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the cleaned job posts dataset\n",
    "df2.to_pickle('../data_local/job_posts_clean_022818.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tue, 27 Feb 2018 18:01:23 -0600 (CST)    33\n",
       "Mon, 26 Feb 2018 18:18:35 -0600 (CST)    33\n",
       "Tue, 27 Feb 2018 13:27:48 -0600 (CST)    20\n",
       "Mon, 26 Feb 2018 13:47:08 -0600 (CST)    20\n",
       "Tue, 27 Feb 2018 01:00:46 +0000 (UTC)    11\n",
       "Wed, 28 Feb 2018 01:24:21 +0000 (UTC)    11\n",
       "Name: email_date, dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.email_date.value_counts()\n",
    "# not using this field for now, just keeping it for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"GeoPhy is a company that offers independent property intelligence for a variety of businesses that invest or are otherwise involved in the real estate market and/or financial sector. GeoPhy provides objective data and analysis that is accessible for business through our online platform.\\n\\nYou will have the opportunity to accelerate our rapidly growing organisation. We're a lean team, so your impact will be felt immediately.\\n\\nWe're already working with some of the largest real estate lenders and investors across the globe, and we believe that our AVM will truly disrupt the commercial real estate industry. Using your machine learning and analytical skills, you will contribute to the development of GeoPhy's core information products. This includes working on the development of our flagship product, the Automated Valuation Model (AVM) that we've developed for the commercial real estate market.\\n\\nWhat you'll be responsible for\\nDeveloping and maintaining predictive valuation algorithms for the commercial real estate market, based on stochastic modelling\\nIdentifying and analysing new data sources to improve model accuracy, closely working with our data sourcing team\\nBringing models to production, in collaboration with the development and data engineering teams\\nWhat we're looking for\\nSomeone who is intellectually curious with steady hands-on experience as a data scientist and a love for statistics\\nDeep knowledge of Python, Matlab, or R\\nThorough knowledge of supervised and unsupervised learning algorithms.\\nIntermediate knowledge of SQL\\nFull working proficiency in English\\nAn independent, proactive and reliable team player\\nAn MSc/PhD degree in Computer Science, Mathematics, Statistics or a related subject\\nBonus points for the following\\nInternational mind set\\nExperience in an Agile organisation\\nKnowledge of global real estate or start up markets\\nExperience with Google Suite, Jira, Slack\\nWhat we give you\\nSalary based on suitability for the role\\nFlexible hours and working location\\nWorking with smart people in an international environment\\nA full-time contract for 12 months to start\\nIf you're convinced you are the right fit and you can’t wait to join our team, we look forward to hearing from you!\\n\\nCommitment: Fixed-term\\n\\nDepartment: Data\\n\\nLocation: New York, US\\n\\nTeam: Data Science\""
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.job_posting[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are looking for Senior Data Scientists with strong mathematical backgrounds to work alongside our engineering teams to build the next generation of retail and commerce models that delight and empower marketers. The ideal candidate is one that has several years of experience researching, building, serving, and maintaining data science models at scale. They have first-hand experience with what works and what doesn’t, and are eager to share this experience with more junior members and guide them through that process. They are also able to and excited to help architect and build out the data science architecture needed to accelerate innovation on models and facilitate serving and maintaining them. Finally, they should be curious and eager to identify and explore the myriad of other products that can be built on our data asset. Our culture emphasizes making good tradeoffs, working as a team, and leaving your ego at the door.\n",
      "\n",
      "First-party data is at the core of everything we build and the Data Science team at Bluecore is at the forefront of exploring innovative and exciting ways to activate that data asset. We’re a dedicated team of data scientists and engineers working together to build powerful models that empower marketers to make the right decisions and engage their customers with personalized content. Our approach to building models is an academic one, starting with a literature search, a baseline, and an iterative process of training and validation to identify the most suitable model that is as simple as possible and as powerful as necessary. We employ a wide variety of models, such as Bayesian models for predicting customer lifetime value, and matrix factorization to identify a customer’s product affinity. Our models operate at scale and crunch through millions of data points to make decisions that have been shown to double revenue and triple reach, and are designed in a flexible manner to generalize across our set of 300+ diverse customers who span industries from apparel to automotive. To explore, build, deploy and maintain models we leverage many tools such as BigQuery, Spark, Cloud SQL, Keras, TensorFlow, Airflow, Google App Engine and Google Compute Engine. Finally, we’re a team that values applied research and constantly exploring the frontier of what’s possible; diving into topics such as topic modeling, restricted Boltzmann machines, recurrent neural networks, convolutional neural networks for image feature extraction, and most recently an extensive dive into differential privacy.\n",
      "\n",
      "Responsibilities\n",
      "Identify new opportunities to leverage our data asset\n",
      "Propose and drive technical initiatives\n",
      "Propose infrastructure to accelerate the pace of model exploration and improve model serving and maintenance\n",
      "Meet with customers to discuss our models and our roadmap\n",
      "Identifying appropriate models/algorithms to solve product requirements\n",
      "Meticulous experimentation to evaluate and compare models\n",
      "Writing internal and external facing documentation describing models and approaches\n",
      "Deploying models to production and maintaining them\n",
      "Requirements\n",
      "PhD or MS in Computer Science, Electrical Engineering, Applied Math, Statistics or other relevant quantitative disciplines, or equivalent industry experience\n",
      "Relevant coursework and experience in the fields of Machine Learning, Statistics, Optimization\n",
      "Deep understanding of statistical/probabilistic analysis and linear algebra\n",
      "3+ years of relevant industry experience, including internships\n",
      "Ability to write production-ready code\n",
      "Experience with Natural Language Processing, Deep Learning, Reinforcement Learning, and/or Optimization\n",
      "Experience with ML at scale\n",
      "Experience with scale computing: Hadoop/Scala/Spark/Cassandra\n",
      "Experience with SQL\n",
      "Benefits\n",
      "Highly competitive compensation package including salary and equity as well as the opportunity to work for one of the fastest growing marketing technology start-ups\n",
      "Comprehensive medical, dental, and vision insurance\n",
      "401(k) plan\n",
      "Monthly fitness stipend for a gym membership or fitness classes\n",
      "Monthly NYC Metro Card\n",
      "Generous Parental Leave & flexible vacation policy\n",
      "At Bluecore we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.\n",
      "\n",
      "Bluecore is a proud equal opportunity employer. We are committed to fair hiring practices and to creating a welcoming environment for all team members. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, disability, age, familial status or veteran status.\n"
     ]
    }
   ],
   "source": [
    "print(df2.job_posting[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(df2)):\n",
    "    if 'equal opportunity employer' in df2.job_posting[i]:\n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "\n",
    "# status employment protected gender applicants race color disability religion opportunity sexual age equal orientation qualified veteran sex employer regard identity demonstrated law marital\n",
    "\n",
    "\n",
    "# job required education master type years salary year location desired minimum include tasks level range highly expert degree bachelor position resume knowledge related duties summary requirements benefits preferred applicants phd present needed need employee employer effectively effective employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "jobpost_blob = TextBlob(df2.job_posting[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['senior data scientists', 'strong mathematical backgrounds', 'engineering teams', 'commerce models', 'empower marketers', 'ideal candidate', 'data science models', 'first-hand experience', 'doesn ’ t', 'junior members', 'data science architecture', 'accelerate innovation', 'data asset', 'first-party', 'data', 'science team', 'bluecore', 'exciting ways', 'data asset', '’ re', 'data scientists', 'powerful models', 'empower marketers', 'right decisions', 'building models', 'literature search', 'iterative process', 'suitable model', 'wide variety', 'bayesian', 'customer lifetime value', 'matrix factorization', 'customer ’ s product affinity', 'data points', 'double revenue', 'flexible manner', '300+ diverse customers', 'span industries', 'bigquery', 'spark', 'cloud sql', 'keras', 'tensorflow', 'airflow', 'google app engine', 'google compute engine', '’ re', '’ s', 'boltzmann', 'recurrent neural networks', 'convolutional neural networks', 'extensive dive', 'differential privacy', 'responsibilities identify', 'new opportunities', 'data asset', 'propose', 'technical initiatives', 'propose', 'model exploration', 'identifying', 'appropriate models/algorithms', 'product requirements', 'meticulous', 'writing', 'deploying', 'requirements phd', 'ms', 'computer', 'electrical engineering', 'applied math', 'statistics', 'relevant quantitative disciplines', 'equivalent industry experience', 'relevant', 'machine learning', 'statistics', 'optimization deep', 'statistical/probabilistic analysis', 'linear algebra 3+ years', 'relevant industry experience', 'ability', 'production-ready code', 'experience', 'language processing', 'deep learning', 'reinforcement learning', 'optimization experience', 'ml', 'experience', 'hadoop/scala/spark/cassandra experience', 'sql benefits highly', 'competitive compensation package', 'technology start-ups', 'comprehensive', 'vision insurance', 'plan monthly fitness stipend', 'gym membership', 'fitness classes monthly', 'nyc metro card generous parental', 'flexible vacation policy', 'bluecore', 'inclusive environment', 'unique perspectives', 'diverse teams', 'strong teams', 'bluecore', 'proud equal opportunity employer', 'team members', 'gender identity', 'sexual orientation', 'national origin', 'familial status', 'veteran status'])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobpost_blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.21752183489025595, subjectivity=0.5575275309485834)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobpost_blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sentiment(polarity=0.21752183489025595, subjectivity=0.5575275309485834)\n",
      "1\n",
      "Sentiment(polarity=0.2888095238095238, subjectivity=0.5622619047619047)\n",
      "2\n",
      "Sentiment(polarity=0.27485096500721506, subjectivity=0.5180242153679654)\n",
      "3\n",
      "Sentiment(polarity=0.1291156462585034, subjectivity=0.46977891156462587)\n",
      "4\n",
      "Sentiment(polarity=0.12274914801510545, subjectivity=0.42007230358294184)\n",
      "5\n",
      "Sentiment(polarity=0.10781750906750909, subjectivity=0.5378483678483678)\n",
      "6\n",
      "Sentiment(polarity=0.20129370629370633, subjectivity=0.4879786879786879)\n",
      "7\n",
      "Sentiment(polarity=0.012500000000000004, subjectivity=0.3125)\n",
      "8\n",
      "Sentiment(polarity=0.20635888501742164, subjectivity=0.40801393728222995)\n",
      "9\n",
      "Sentiment(polarity=0.2459273182957393, subjectivity=0.43743734335839596)\n",
      "10\n",
      "Sentiment(polarity=0.2243340380549683, subjectivity=0.6019403335682404)\n",
      "11\n",
      "Sentiment(polarity=0.07249442476715205, subjectivity=0.3805129214220124)\n",
      "12\n",
      "Sentiment(polarity=0.1015151515151515, subjectivity=0.49727272727272726)\n",
      "13\n",
      "Sentiment(polarity=0.19537619047619045, subjectivity=0.481614652014652)\n",
      "14\n",
      "Sentiment(polarity=0.15612962174437578, subjectivity=0.3786112293899179)\n",
      "15\n",
      "Sentiment(polarity=0.14070277240490006, subjectivity=0.3927917472598323)\n",
      "16\n",
      "Sentiment(polarity=0.10062615955473099, subjectivity=0.38179241393527114)\n",
      "17\n",
      "Sentiment(polarity=0.18644681508967226, subjectivity=0.41513790970933834)\n",
      "18\n",
      "Sentiment(polarity=0.13245967741935485, subjectivity=0.3351670506912442)\n",
      "19\n",
      "Sentiment(polarity=0.13505821764442455, subjectivity=0.40601507687714583)\n",
      "20\n",
      "Sentiment(polarity=0.17033898305084746, subjectivity=0.3479015334947538)\n",
      "21\n",
      "Sentiment(polarity=0.07574852589938798, subjectivity=0.4910319637259292)\n",
      "22\n",
      "Sentiment(polarity=0.2607303503787879, subjectivity=0.4695904356060608)\n",
      "23\n",
      "Sentiment(polarity=0.07151589789520821, subjectivity=0.35437975817286155)\n",
      "24\n",
      "Sentiment(polarity=0.08666666666666666, subjectivity=0.4677625152625153)\n",
      "25\n",
      "Sentiment(polarity=0.1002164502164502, subjectivity=0.4652056277056277)\n",
      "26\n",
      "Sentiment(polarity=0.1903138528138528, subjectivity=0.4639610389610391)\n",
      "27\n",
      "Sentiment(polarity=0.2465969215969216, subjectivity=0.5373556998556999)\n",
      "28\n",
      "Sentiment(polarity=0.07047468035840128, subjectivity=0.3311411456760293)\n",
      "29\n",
      "Sentiment(polarity=0.17785714285714288, subjectivity=0.4159523809523809)\n",
      "30\n",
      "Sentiment(polarity=0.05349412492269635, subjectivity=0.3283704390847248)\n",
      "31\n",
      "Sentiment(polarity=0.12541798941798943, subjectivity=0.46077248677248683)\n",
      "32\n",
      "Sentiment(polarity=0.06153846153846154, subjectivity=0.35769230769230764)\n",
      "33\n",
      "Sentiment(polarity=0.2888095238095238, subjectivity=0.5622619047619047)\n",
      "34\n",
      "Sentiment(polarity=0.23411330049261078, subjectivity=0.479720853858785)\n",
      "35\n",
      "Sentiment(polarity=0.21396428571428575, subjectivity=0.502970238095238)\n",
      "36\n",
      "Sentiment(polarity=0.14327380952380955, subjectivity=0.44523809523809527)\n",
      "37\n",
      "Sentiment(polarity=0.26666666666666666, subjectivity=0.5166666666666666)\n",
      "38\n",
      "Sentiment(polarity=0.18139952153110048, subjectivity=0.5006237183868762)\n",
      "39\n",
      "Sentiment(polarity=0.10588235294117647, subjectivity=0.3205882352941176)\n",
      "40\n",
      "Sentiment(polarity=0.2298616830324148, subjectivity=0.5248157533523387)\n",
      "41\n",
      "Sentiment(polarity=0.27485096500721506, subjectivity=0.5180242153679654)\n",
      "42\n",
      "Sentiment(polarity=0.1337620605889837, subjectivity=0.38077452462067857)\n",
      "43\n",
      "Sentiment(polarity=0.29536402990948446, subjectivity=0.5398954917136736)\n",
      "44\n",
      "Sentiment(polarity=0.06153846153846154, subjectivity=0.35769230769230764)\n",
      "45\n",
      "Sentiment(polarity=0.14181255526083114, subjectivity=0.5122041177213591)\n",
      "46\n",
      "Sentiment(polarity=0.25997538409303117, subjectivity=0.46788090993973347)\n",
      "47\n",
      "Sentiment(polarity=0.1824550763701707, subjectivity=0.4306064690026955)\n",
      "48\n",
      "Sentiment(polarity=0.05302579365079365, subjectivity=0.31651785714285713)\n",
      "49\n",
      "Sentiment(polarity=0.18946540880503143, subjectivity=0.46833333333333343)\n",
      "50\n",
      "Sentiment(polarity=0.2486000558581204, subjectivity=0.5576351068286552)\n",
      "51\n",
      "Sentiment(polarity=0.20708333333333337, subjectivity=0.4224999999999999)\n",
      "52\n",
      "Sentiment(polarity=0.22399891774891775, subjectivity=0.5019209956709957)\n",
      "53\n",
      "Sentiment(polarity=0.2888095238095238, subjectivity=0.5622619047619047)\n",
      "54\n",
      "Sentiment(polarity=0.05179063360881542, subjectivity=0.38071625344352616)\n",
      "55\n",
      "Sentiment(polarity=0.1291156462585034, subjectivity=0.46977891156462587)\n",
      "56\n",
      "Sentiment(polarity=0.17860962566844915, subjectivity=0.5796791443850267)\n",
      "57\n",
      "Sentiment(polarity=0.26176767676767676, subjectivity=0.5278030303030302)\n",
      "58\n",
      "Sentiment(polarity=0.249036041477902, subjectivity=0.4483911437399809)\n",
      "59\n",
      "Sentiment(polarity=0.16459156785243736, subjectivity=0.4541501976284586)\n",
      "60\n",
      "Sentiment(polarity=0.1692067736185383, subjectivity=0.4469474153297683)\n",
      "61\n",
      "Sentiment(polarity=0.17777777777777778, subjectivity=0.41111111111111115)\n",
      "62\n",
      "Sentiment(polarity=0.209755969836615, subjectivity=0.5031996229576874)\n",
      "63\n",
      "Sentiment(polarity=0.2714468864468864, subjectivity=0.5313919413919413)\n",
      "64\n",
      "Sentiment(polarity=0.12358757062146893, subjectivity=0.3313357546408393)\n",
      "65\n",
      "Sentiment(polarity=0.21491228070175442, subjectivity=0.5741228070175438)\n",
      "66\n",
      "Sentiment(polarity=0.22905303030303034, subjectivity=0.39644412878787877)\n",
      "67\n",
      "Sentiment(polarity=0.16269155844155847, subjectivity=0.5359718614718614)\n",
      "68\n",
      "Sentiment(polarity=0.11818181818181817, subjectivity=0.4041958041958042)\n",
      "69\n",
      "Sentiment(polarity=0.09294705294705295, subjectivity=0.4065959965959966)\n",
      "70\n",
      "Sentiment(polarity=0.05179063360881542, subjectivity=0.38071625344352616)\n",
      "71\n",
      "Sentiment(polarity=0.21978787878787875, subjectivity=0.5251818181818181)\n",
      "72\n",
      "Sentiment(polarity=0.29597222222222225, subjectivity=0.6234722222222222)\n",
      "73\n",
      "Sentiment(polarity=0.0816549314139676, subjectivity=0.40603418669683716)\n",
      "74\n",
      "Sentiment(polarity=0.06373626373626373, subjectivity=0.3967032967032967)\n",
      "75\n",
      "Sentiment(polarity=0.08550644431326247, subjectivity=0.26697043486816224)\n",
      "76\n",
      "Sentiment(polarity=0.19020923520923522, subjectivity=0.5277117327117328)\n",
      "77\n",
      "Sentiment(polarity=0.15729864433811808, subjectivity=0.4607834928229664)\n",
      "78\n",
      "Sentiment(polarity=0.16583333333333333, subjectivity=0.4141666666666667)\n",
      "79\n",
      "Sentiment(polarity=0.07254901960784314, subjectivity=0.5343137254901961)\n",
      "80\n",
      "Sentiment(polarity=0.16791666666666666, subjectivity=0.5044047619047619)\n",
      "81\n",
      "Sentiment(polarity=0.18078382610297503, subjectivity=0.3801169752233582)\n",
      "82\n",
      "Sentiment(polarity=0.0876126126126126, subjectivity=0.40585585585585593)\n",
      "83\n",
      "Sentiment(polarity=0.1062905844155844, subjectivity=0.3223823051948052)\n",
      "84\n",
      "Sentiment(polarity=0.013229166666666667, subjectivity=0.2607291666666667)\n",
      "85\n",
      "Sentiment(polarity=0.16881534090909092, subjectivity=0.41626515151515164)\n",
      "86\n",
      "Sentiment(polarity=0.15123376623376625, subjectivity=0.4688195138195138)\n",
      "87\n",
      "Sentiment(polarity=0.1268272005772006, subjectivity=0.297056277056277)\n",
      "88\n",
      "Sentiment(polarity=0.05556457431457431, subjectivity=0.48101551226551237)\n",
      "89\n",
      "Sentiment(polarity=0.1346699134199134, subjectivity=0.41154852092352084)\n",
      "90\n",
      "Sentiment(polarity=0.10170391623879994, subjectivity=0.35976794523306144)\n",
      "91\n",
      "Sentiment(polarity=0.22965159840159843, subjectivity=0.5299034299034299)\n",
      "92\n",
      "Sentiment(polarity=0.08666666666666666, subjectivity=0.4677625152625153)\n",
      "93\n",
      "Sentiment(polarity=0.21098484848484853, subjectivity=0.4984848484848486)\n",
      "94\n",
      "Sentiment(polarity=0.22944550547491724, subjectivity=0.49266849163907983)\n",
      "95\n",
      "Sentiment(polarity=0.1002164502164502, subjectivity=0.4652056277056277)\n",
      "96\n",
      "Sentiment(polarity=0.1397463768115942, subjectivity=0.4596066252587992)\n",
      "97\n",
      "Sentiment(polarity=0.28117216117216115, subjectivity=0.49208791208791214)\n",
      "98\n",
      "Sentiment(polarity=0.11232575757575758, subjectivity=0.432655303030303)\n",
      "99\n",
      "Sentiment(polarity=0.15273224043715847, subjectivity=0.3597189695550351)\n",
      "100\n",
      "Sentiment(polarity=0.265, subjectivity=0.71)\n",
      "101\n",
      "Sentiment(polarity=0.08106060606060607, subjectivity=0.4193722943722944)\n",
      "102\n",
      "Sentiment(polarity=0.1731437754165027, subjectivity=0.37903384494293585)\n",
      "103\n",
      "Sentiment(polarity=0.13910353535353537, subjectivity=0.4598953823953824)\n",
      "104\n",
      "Sentiment(polarity=0.05000000000000001, subjectivity=0.4274216524216524)\n",
      "105\n",
      "Sentiment(polarity=0.21763436249285306, subjectivity=0.45565751858204695)\n",
      "106\n",
      "Sentiment(polarity=0.178656462585034, subjectivity=0.5464495028780743)\n",
      "107\n",
      "Sentiment(polarity=0.20679048892284185, subjectivity=0.42491787624140565)\n",
      "108\n",
      "Sentiment(polarity=0.19687500000000005, subjectivity=0.38025793650793654)\n",
      "109\n",
      "Sentiment(polarity=0.1589955106621774, subjectivity=0.5176599326599326)\n",
      "110\n",
      "Sentiment(polarity=0.15156034024455078, subjectivity=0.3995427963849017)\n",
      "111\n",
      "Sentiment(polarity=0.17932162534435264, subjectivity=0.4918044077134987)\n",
      "112\n",
      "Sentiment(polarity=0.14232954545454546, subjectivity=0.47945075757575756)\n",
      "113\n",
      "Sentiment(polarity=0.3194444444444444, subjectivity=0.522718253968254)\n",
      "114\n",
      "Sentiment(polarity=0.22034473656424874, subjectivity=0.5162327103790519)\n",
      "115\n",
      "Sentiment(polarity=0.26176767676767676, subjectivity=0.5278030303030302)\n",
      "116\n",
      "Sentiment(polarity=0.1692067736185383, subjectivity=0.4469474153297683)\n",
      "117\n",
      "Sentiment(polarity=0.14559884559884556, subjectivity=0.41601731601731606)\n",
      "118\n",
      "Sentiment(polarity=0.23721372713308195, subjectivity=0.4817553414327609)\n",
      "119\n",
      "Sentiment(polarity=0.22695574162679427, subjectivity=0.43708931419457747)\n",
      "120\n",
      "Sentiment(polarity=0.23523113790970934, subjectivity=0.38301252319109463)\n",
      "121\n",
      "Sentiment(polarity=0.13282828282828282, subjectivity=0.38072390572390563)\n",
      "122\n",
      "Sentiment(polarity=0.15123376623376625, subjectivity=0.4688195138195138)\n",
      "123\n",
      "Sentiment(polarity=0.10029109371021136, subjectivity=0.3728181499872677)\n",
      "124\n",
      "Sentiment(polarity=0.23183557513914657, subjectivity=0.5169391620284478)\n",
      "125\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "126\n",
      "Sentiment(polarity=0.15532292152292154, subjectivity=0.4215177933177932)\n",
      "127\n",
      "Sentiment(polarity=0.08631136305554911, subjectivity=0.3798014001502374)\n",
      "128\n",
      "Sentiment(polarity=0.13893864755933721, subjectivity=0.4602963128825198)\n",
      "129\n",
      "Sentiment(polarity=0.1188582251082251, subjectivity=0.38963744588744587)\n",
      "130\n",
      "Sentiment(polarity=0.19020923520923522, subjectivity=0.5277117327117328)\n",
      "131\n",
      "Sentiment(polarity=0.13838744588744586, subjectivity=0.3737492269635127)\n",
      "132\n",
      "Sentiment(polarity=0.15729864433811808, subjectivity=0.4607834928229664)\n",
      "133\n",
      "Sentiment(polarity=0.08512396694214878, subjectivity=0.518595041322314)\n",
      "134\n",
      "Sentiment(polarity=0.12061885482938113, subjectivity=0.48015953344900725)\n",
      "135\n",
      "Sentiment(polarity=0.13613636363636364, subjectivity=0.4067045454545455)\n",
      "136\n",
      "Sentiment(polarity=0.2845238095238096, subjectivity=0.47087301587301583)\n",
      "137\n",
      "Sentiment(polarity=0.18214285714285708, subjectivity=0.45615079365079375)\n",
      "138\n",
      "Sentiment(polarity=0.16760789715335167, subjectivity=0.4856650924832744)\n",
      "139\n",
      "Sentiment(polarity=0.013229166666666667, subjectivity=0.2607291666666667)\n",
      "140\n",
      "Sentiment(polarity=0.14035794452461123, subjectivity=0.36774591149591146)\n",
      "141\n",
      "Sentiment(polarity=0.0466403162055336, subjectivity=0.46416337285902487)\n",
      "142\n",
      "Sentiment(polarity=0.16791666666666666, subjectivity=0.5044047619047619)\n",
      "143\n",
      "Sentiment(polarity=0.2516717325227964, subjectivity=0.5983282674772038)\n",
      "144\n",
      "Sentiment(polarity=0.1346699134199134, subjectivity=0.41154852092352084)\n",
      "145\n",
      "Sentiment(polarity=0.18238636363636362, subjectivity=0.4677083333333335)\n",
      "146\n",
      "Sentiment(polarity=0.10630050505050506, subjectivity=0.3439646464646463)\n",
      "147\n",
      "Sentiment(polarity=0.16583333333333333, subjectivity=0.4141666666666667)\n",
      "148\n",
      "Sentiment(polarity=0.07254901960784314, subjectivity=0.5343137254901961)\n",
      "149\n",
      "Sentiment(polarity=0.21341991341991343, subjectivity=0.45108225108225114)\n",
      "150\n",
      "Sentiment(polarity=0.24841269841269842, subjectivity=0.5511904761904762)\n",
      "151\n",
      "Sentiment(polarity=0.060366568914956016, subjectivity=0.3428968343484472)\n",
      "152\n",
      "Sentiment(polarity=0.15123376623376625, subjectivity=0.4688195138195138)\n",
      "153\n",
      "Sentiment(polarity=0.12428866587957497, subjectivity=0.46588895952532317)\n",
      "154\n",
      "Sentiment(polarity=0.0773452380952381, subjectivity=0.4111071428571428)\n",
      "155\n",
      "Sentiment(polarity=0.05556457431457431, subjectivity=0.48101551226551237)\n",
      "156\n",
      "Sentiment(polarity=0.17272727272727276, subjectivity=0.4595811051693403)\n",
      "157\n",
      "Sentiment(polarity=0.23721372713308195, subjectivity=0.4817553414327609)\n",
      "158\n",
      "Sentiment(polarity=0.18078382610297503, subjectivity=0.3801169752233582)\n",
      "159\n",
      "Sentiment(polarity=0.14559884559884556, subjectivity=0.41601731601731606)\n",
      "160\n",
      "Sentiment(polarity=0.23721372713308195, subjectivity=0.4817553414327609)\n",
      "161\n",
      "Sentiment(polarity=0.11818181818181817, subjectivity=0.4041958041958042)\n",
      "162\n",
      "Sentiment(polarity=0.13772607022607025, subjectivity=0.5386358086358085)\n",
      "163\n",
      "Sentiment(polarity=0.3194444444444444, subjectivity=0.522718253968254)\n",
      "164\n",
      "Sentiment(polarity=0.1752635046113307, subjectivity=0.5076416337285903)\n",
      "165\n",
      "Sentiment(polarity=0.1964222052457347, subjectivity=0.4040520753756048)\n",
      "166\n",
      "Sentiment(polarity=0.25480668756530833, subjectivity=0.5130234707820914)\n",
      "167\n",
      "Sentiment(polarity=0.21818491032776743, subjectivity=0.5441991341991341)\n",
      "168\n",
      "Sentiment(polarity=0.29609217171717167, subjectivity=0.5204306958473626)\n",
      "169\n",
      "Sentiment(polarity=0.1709777578743096, subjectivity=0.3996453450763795)\n",
      "170\n",
      "Sentiment(polarity=0.21491228070175442, subjectivity=0.5741228070175438)\n",
      "171\n",
      "Sentiment(polarity=0.19680555555555557, subjectivity=0.37444444444444447)\n",
      "172\n",
      "Sentiment(polarity=0.31338888888888883, subjectivity=0.5649444444444445)\n",
      "173\n",
      "Sentiment(polarity=0.1560997910135841, subjectivity=0.36482497387669804)\n",
      "174\n",
      "Sentiment(polarity=0.11444281524926685, subjectivity=0.3959921798631476)\n",
      "175\n",
      "Sentiment(polarity=0.3094415584415584, subjectivity=0.543056895485467)\n",
      "176\n",
      "Sentiment(polarity=0.255467007739735, subjectivity=0.5224321133412042)\n",
      "177\n",
      "Sentiment(polarity=0.18951219512195122, subjectivity=0.39759053954175905)\n",
      "178\n",
      "Sentiment(polarity=0.1768256333830104, subjectivity=0.46585941381023355)\n",
      "179\n",
      "Sentiment(polarity=0.20312520812520815, subjectivity=0.4368681318681318)\n",
      "180\n",
      "Sentiment(polarity=0.15518419489007726, subjectivity=0.4182454800101859)\n",
      "181\n",
      "Sentiment(polarity=0.21972698684962838, subjectivity=0.42975557461406516)\n",
      "182\n",
      "Sentiment(polarity=0.18619453463203464, subjectivity=0.4780945616883117)\n",
      "183\n",
      "Sentiment(polarity=0.1390630797773655, subjectivity=0.400448361162647)\n",
      "184\n",
      "Sentiment(polarity=0.24349350649350648, subjectivity=0.6154891774891775)\n",
      "185\n",
      "Sentiment(polarity=0.147490316700843, subjectivity=0.48674299384825703)\n",
      "186\n",
      "Sentiment(polarity=0.11507142857142857, subjectivity=0.404142857142857)\n",
      "187\n",
      "Sentiment(polarity=0.08705808080808079, subjectivity=0.41773989898989905)\n",
      "188\n",
      "Sentiment(polarity=0.15729864433811808, subjectivity=0.4607834928229664)\n",
      "189\n",
      "Sentiment(polarity=0.16760789715335167, subjectivity=0.4856650924832744)\n",
      "190\n",
      "Sentiment(polarity=0.18192137320044297, subjectivity=0.608970099667774)\n",
      "191\n",
      "Sentiment(polarity=0.17209706959706964, subjectivity=0.4358424908424908)\n",
      "192\n",
      "Sentiment(polarity=0.2688775510204081, subjectivity=0.5663265306122449)\n",
      "193\n",
      "Sentiment(polarity=0.1188582251082251, subjectivity=0.38963744588744587)\n",
      "194\n",
      "Sentiment(polarity=0.006249999999999992, subjectivity=0.53125)\n",
      "195\n",
      "Sentiment(polarity=0.12291756854256854, subjectivity=0.4654279401154402)\n",
      "196\n",
      "Sentiment(polarity=0.1346699134199134, subjectivity=0.41154852092352084)\n",
      "197\n",
      "Sentiment(polarity=0.13838744588744586, subjectivity=0.3737492269635127)\n",
      "198\n",
      "Sentiment(polarity=0.15009300595238095, subjectivity=0.4820684523809524)\n",
      "199\n",
      "Sentiment(polarity=0.21294122544122543, subjectivity=0.4699383949383949)\n",
      "200\n",
      "Sentiment(polarity=0.22863636363636367, subjectivity=0.5112878787878788)\n",
      "201\n",
      "Sentiment(polarity=0.18878787878787878, subjectivity=0.5490909090909091)\n",
      "202\n",
      "Sentiment(polarity=0.2559404388714734, subjectivity=0.4569122257053292)\n",
      "203\n",
      "Sentiment(polarity=0.08181818181818182, subjectivity=0.27259499759499756)\n",
      "204\n",
      "Sentiment(polarity=0.10592778433687523, subjectivity=0.4852715466351831)\n",
      "205\n",
      "Sentiment(polarity=0.09191836734693878, subjectivity=0.4872993197278911)\n",
      "206\n",
      "Sentiment(polarity=0.059145021645021646, subjectivity=0.4344581647860336)\n",
      "207\n",
      "Sentiment(polarity=0.18214285714285708, subjectivity=0.45615079365079375)\n",
      "208\n",
      "Sentiment(polarity=0.13613636363636364, subjectivity=0.4067045454545455)\n",
      "209\n",
      "Sentiment(polarity=0.013229166666666667, subjectivity=0.2607291666666667)\n",
      "210\n",
      "Sentiment(polarity=0.18546608946608945, subjectivity=0.4960761183261184)\n",
      "211\n",
      "Sentiment(polarity=0.1638751352813853, subjectivity=0.49113140331890337)\n",
      "212\n",
      "Sentiment(polarity=0.16791666666666666, subjectivity=0.5044047619047619)\n",
      "213\n",
      "Sentiment(polarity=0.2298616830324148, subjectivity=0.5248157533523387)\n",
      "214\n",
      "Sentiment(polarity=0.23980392156862743, subjectivity=0.42196078431372547)\n",
      "215\n",
      "Sentiment(polarity=0.16870978120978122, subjectivity=0.41289146289146295)\n",
      "216\n",
      "Sentiment(polarity=0.1390630797773655, subjectivity=0.400448361162647)\n",
      "217\n",
      "Sentiment(polarity=0.24349350649350648, subjectivity=0.6154891774891775)\n",
      "218\n",
      "Sentiment(polarity=0.147490316700843, subjectivity=0.48674299384825703)\n",
      "219\n",
      "Sentiment(polarity=0.11507142857142857, subjectivity=0.404142857142857)\n",
      "220\n",
      "Sentiment(polarity=0.08705808080808079, subjectivity=0.41773989898989905)\n",
      "221\n",
      "Sentiment(polarity=0.15729864433811808, subjectivity=0.4607834928229664)\n",
      "222\n",
      "Sentiment(polarity=0.16760789715335167, subjectivity=0.4856650924832744)\n",
      "223\n",
      "Sentiment(polarity=0.18192137320044297, subjectivity=0.608970099667774)\n",
      "224\n",
      "Sentiment(polarity=0.17209706959706964, subjectivity=0.4358424908424908)\n",
      "225\n",
      "Sentiment(polarity=0.2688775510204081, subjectivity=0.5663265306122449)\n",
      "226\n",
      "Sentiment(polarity=0.1188582251082251, subjectivity=0.38963744588744587)\n",
      "227\n",
      "Sentiment(polarity=0.12291756854256854, subjectivity=0.4654279401154402)\n",
      "228\n",
      "Sentiment(polarity=0.006249999999999992, subjectivity=0.53125)\n",
      "229\n",
      "Sentiment(polarity=0.1346699134199134, subjectivity=0.41154852092352084)\n",
      "230\n",
      "Sentiment(polarity=0.13838744588744586, subjectivity=0.3737492269635127)\n",
      "231\n",
      "Sentiment(polarity=0.15009300595238095, subjectivity=0.4820684523809524)\n",
      "232\n",
      "Sentiment(polarity=0.21294122544122543, subjectivity=0.4699383949383949)\n",
      "233\n",
      "Sentiment(polarity=0.22863636363636367, subjectivity=0.5112878787878788)\n",
      "234\n",
      "Sentiment(polarity=0.18878787878787878, subjectivity=0.5490909090909091)\n",
      "235\n",
      "Sentiment(polarity=0.08181818181818182, subjectivity=0.27259499759499756)\n",
      "236\n",
      "Sentiment(polarity=0.10592778433687523, subjectivity=0.4852715466351831)\n",
      "237\n",
      "Sentiment(polarity=0.2559404388714734, subjectivity=0.4569122257053292)\n",
      "238\n",
      "Sentiment(polarity=0.18214285714285708, subjectivity=0.45615079365079375)\n",
      "239\n",
      "Sentiment(polarity=0.09191836734693878, subjectivity=0.4872993197278911)\n",
      "240\n",
      "Sentiment(polarity=0.18546608946608945, subjectivity=0.4960761183261184)\n",
      "241\n",
      "Sentiment(polarity=0.059145021645021646, subjectivity=0.4344581647860336)\n",
      "242\n",
      "Sentiment(polarity=0.013229166666666667, subjectivity=0.2607291666666667)\n",
      "243\n",
      "Sentiment(polarity=0.13613636363636364, subjectivity=0.4067045454545455)\n",
      "244\n",
      "Sentiment(polarity=0.1638751352813853, subjectivity=0.49113140331890337)\n",
      "245\n",
      "Sentiment(polarity=0.08512396694214878, subjectivity=0.518595041322314)\n",
      "246\n",
      "Sentiment(polarity=0.21294122544122543, subjectivity=0.4699383949383949)\n",
      "247\n",
      "Sentiment(polarity=0.2502998737373738, subjectivity=0.5182449494949495)\n",
      "248\n",
      "Sentiment(polarity=0.2688775510204081, subjectivity=0.5663265306122449)\n",
      "249\n",
      "Sentiment(polarity=0.13775703463203465, subjectivity=0.5405469530469529)\n",
      "250\n",
      "Sentiment(polarity=0.19687500000000005, subjectivity=0.38025793650793654)\n",
      "251\n",
      "Sentiment(polarity=0.20679048892284185, subjectivity=0.42491787624140565)\n",
      "252\n",
      "Sentiment(polarity=0.08512396694214878, subjectivity=0.518595041322314)\n",
      "253\n",
      "Sentiment(polarity=0.1274851190476191, subjectivity=0.49543154761904773)\n",
      "254\n",
      "Sentiment(polarity=0.3194444444444444, subjectivity=0.522718253968254)\n",
      "255\n",
      "Sentiment(polarity=0.1752635046113307, subjectivity=0.5076416337285903)\n",
      "256\n",
      "Sentiment(polarity=0.12302188552188555, subjectivity=0.520412457912458)\n",
      "257\n",
      "Sentiment(polarity=0.10678122690317818, subjectivity=0.3861771561771557)\n",
      "258\n",
      "Sentiment(polarity=0.1964222052457347, subjectivity=0.4040520753756048)\n",
      "259\n",
      "Sentiment(polarity=0.2179951298701299, subjectivity=0.4910551948051948)\n",
      "260\n",
      "Sentiment(polarity=0.39189342403628113, subjectivity=0.4942176870748301)\n",
      "261\n",
      "Sentiment(polarity=0.15157974837662339, subjectivity=0.4762504509379508)\n",
      "262\n",
      "Sentiment(polarity=0.1541950113378685, subjectivity=0.4435752078609221)\n",
      "263\n",
      "Sentiment(polarity=0.2757875103619785, subjectivity=0.4791701206594823)\n",
      "264\n",
      "Sentiment(polarity=0.22626511419614873, subjectivity=0.5423386000972208)\n",
      "265\n",
      "Sentiment(polarity=0.16958072791406126, subjectivity=0.4324354657687991)\n",
      "266\n",
      "Sentiment(polarity=0.13232359307359307, subjectivity=0.44591233766233773)\n",
      "267\n",
      "Sentiment(polarity=0.2688775510204081, subjectivity=0.5663265306122449)\n",
      "268\n",
      "Sentiment(polarity=0.2900493025493026, subjectivity=0.493903318903319)\n",
      "269\n",
      "Sentiment(polarity=0.2298616830324148, subjectivity=0.5248157533523387)\n",
      "270\n",
      "Sentiment(polarity=0.17234268147311627, subjectivity=0.37921105464583715)\n",
      "271\n",
      "Sentiment(polarity=0.16870978120978122, subjectivity=0.41289146289146295)\n",
      "272\n",
      "Sentiment(polarity=0.18961926052089992, subjectivity=0.3701955148676459)\n",
      "273\n",
      "Sentiment(polarity=0.07966269841269841, subjectivity=0.376686507936508)\n",
      "274\n",
      "Sentiment(polarity=0.1642619047619048, subjectivity=0.3648383888838435)\n",
      "275\n",
      "Sentiment(polarity=0.09018073593073593, subjectivity=0.35768181818181816)\n",
      "276\n",
      "Sentiment(polarity=0.19154539904539902, subjectivity=0.48193806193806193)\n",
      "277\n",
      "Sentiment(polarity=0.17145562770562775, subjectivity=0.49406340187590186)\n",
      "278\n",
      "Sentiment(polarity=0.1736038961038961, subjectivity=0.4913124375624375)\n",
      "279\n",
      "Sentiment(polarity=0.13024193548387095, subjectivity=0.43575268817204293)\n",
      "280\n",
      "Sentiment(polarity=0.1659090909090909, subjectivity=0.2522727272727272)\n",
      "281\n",
      "Sentiment(polarity=0.22612612612612618, subjectivity=0.42409909909909893)\n",
      "282\n",
      "Sentiment(polarity=0.15163690476190478, subjectivity=0.39095238095238094)\n",
      "283\n",
      "Sentiment(polarity=0.31329280648429586, subjectivity=0.5980547112462007)\n",
      "284\n",
      "Sentiment(polarity=0.1390630797773655, subjectivity=0.400448361162647)\n",
      "285\n",
      "Sentiment(polarity=0.2845238095238096, subjectivity=0.47087301587301583)\n",
      "286\n",
      "Sentiment(polarity=0.12291756854256854, subjectivity=0.4654279401154402)\n",
      "287\n",
      "Sentiment(polarity=0.21030126336248786, subjectivity=0.5064868804664725)\n",
      "288\n",
      "Sentiment(polarity=-0.01428571428571429, subjectivity=0.31587301587301586)\n",
      "289\n",
      "Sentiment(polarity=0.10352586010480751, subjectivity=0.522582023239918)\n",
      "290\n",
      "Sentiment(polarity=0.20462070874861568, subjectivity=0.4455177187153931)\n",
      "291\n",
      "Sentiment(polarity=0.18214285714285708, subjectivity=0.45615079365079375)\n",
      "292\n",
      "Sentiment(polarity=0.17391082087104814, subjectivity=0.42363521579430663)\n",
      "293\n",
      "Sentiment(polarity=-0.004166666666666661, subjectivity=0.3583333333333334)\n",
      "294\n",
      "Sentiment(polarity=0.1430555555555556, subjectivity=0.3784722222222222)\n",
      "295\n",
      "Sentiment(polarity=0.10049603174603174, subjectivity=0.459077380952381)\n",
      "296\n",
      "Sentiment(polarity=0.10346153846153844, subjectivity=0.4125641025641025)\n",
      "297\n",
      "Sentiment(polarity=0.11797440002919457, subjectivity=0.42083108672149766)\n",
      "298\n",
      "Sentiment(polarity=0.1242318098568099, subjectivity=0.40306797369297365)\n",
      "299\n",
      "Sentiment(polarity=0.09596049783549784, subjectivity=0.4780925324675324)\n",
      "300\n",
      "Sentiment(polarity=0.1638751352813853, subjectivity=0.49113140331890337)\n",
      "301\n",
      "Sentiment(polarity=0.2055295693007557, subjectivity=0.4591367671876145)\n",
      "302\n",
      "Sentiment(polarity=0.17188995215311006, subjectivity=0.4195621629832155)\n",
      "303\n",
      "Sentiment(polarity=0.11507142857142857, subjectivity=0.404142857142857)\n",
      "304\n",
      "Sentiment(polarity=0.15275252525252522, subjectivity=0.416053391053391)\n",
      "305\n",
      "Sentiment(polarity=0.21848811565792703, subjectivity=0.4999019848076451)\n",
      "306\n",
      "Sentiment(polarity=0.13838744588744586, subjectivity=0.3737492269635127)\n",
      "307\n",
      "Sentiment(polarity=0.12458018068887634, subjectivity=0.32777348014304536)\n",
      "308\n",
      "Sentiment(polarity=0.19804449648711944, subjectivity=0.38519125683060107)\n",
      "309\n",
      "Sentiment(polarity=0.1665343915343915, subjectivity=0.36779100529100534)\n",
      "310\n",
      "Sentiment(polarity=0.13710437710437712, subjectivity=0.44137085137085136)\n",
      "311\n",
      "Sentiment(polarity=0.08498359167714008, subjectivity=0.445630149420472)\n",
      "312\n",
      "Sentiment(polarity=0.1346699134199134, subjectivity=0.41154852092352084)\n",
      "313\n",
      "Sentiment(polarity=0.13725016118633138, subjectivity=0.42812701482914245)\n",
      "314\n",
      "Sentiment(polarity=0.11818181818181817, subjectivity=0.4041958041958042)\n",
      "315\n",
      "Sentiment(polarity=0.2298616830324148, subjectivity=0.5248157533523387)\n",
      "316\n",
      "Sentiment(polarity=0.2900493025493026, subjectivity=0.493903318903319)\n",
      "317\n",
      "Sentiment(polarity=0.13775703463203465, subjectivity=0.5405469530469529)\n",
      "318\n",
      "Sentiment(polarity=0.19687500000000005, subjectivity=0.38025793650793654)\n",
      "319\n",
      "Sentiment(polarity=0.2134090909090909, subjectivity=0.4511363636363637)\n",
      "320\n",
      "Sentiment(polarity=0.12728787878787876, subjectivity=0.454681818181818)\n",
      "321\n",
      "Sentiment(polarity=0.3194444444444444, subjectivity=0.522718253968254)\n",
      "322\n",
      "Sentiment(polarity=0.18523605932021772, subjectivity=0.46044190133299034)\n",
      "323\n",
      "Sentiment(polarity=0.20593485879200166, subjectivity=0.5432182367896653)\n",
      "324\n",
      "Sentiment(polarity=0.08153846153846155, subjectivity=0.3876923076923076)\n",
      "325\n",
      "Sentiment(polarity=0.13283139274518582, subjectivity=0.4280261405261404)\n",
      "326\n",
      "Sentiment(polarity=0.10653732446415379, subjectivity=0.3849576439820338)\n",
      "327\n",
      "Sentiment(polarity=0.1692067736185383, subjectivity=0.4469474153297683)\n",
      "328\n",
      "Sentiment(polarity=0.16927214452214454, subjectivity=0.4646779054279054)\n",
      "329\n",
      "Sentiment(polarity=0.1858075258075258, subjectivity=0.56002997002997)\n",
      "330\n",
      "Sentiment(polarity=0.10049603174603174, subjectivity=0.459077380952381)\n",
      "331\n",
      "Sentiment(polarity=0.20021825396825396, subjectivity=0.49918650793650793)\n",
      "332\n",
      "Sentiment(polarity=0.043217552887364216, subjectivity=0.3564087641917831)\n",
      "333\n",
      "Sentiment(polarity=0.21294122544122543, subjectivity=0.4699383949383949)\n",
      "334\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "335\n",
      "Sentiment(polarity=0.2277435064935065, subjectivity=0.47759740259740263)\n",
      "336\n",
      "Sentiment(polarity=0.10230654761904762, subjectivity=0.3436755952380952)\n",
      "337\n",
      "Sentiment(polarity=0.14340909090909093, subjectivity=0.5471818181818183)\n",
      "338\n",
      "Sentiment(polarity=0.10759887005649717, subjectivity=0.4021468926553671)\n",
      "339\n",
      "Sentiment(polarity=0.07842412217412216, subjectivity=0.3597034909534909)\n",
      "340\n",
      "Sentiment(polarity=0.15863365800865803, subjectivity=0.41656655844155843)\n",
      "341\n",
      "Sentiment(polarity=0.10943526170798898, subjectivity=0.281198347107438)\n",
      "342\n",
      "Sentiment(polarity=0.07202797202797201, subjectivity=0.46317016317016313)\n",
      "343\n",
      "Sentiment(polarity=0.11293073593073592, subjectivity=0.44613419913419927)\n",
      "344\n",
      "Sentiment(polarity=0.19005928853754944, subjectivity=0.5312845849802371)\n",
      "345\n",
      "Sentiment(polarity=0.0988020081770082, subjectivity=0.40599071067821063)\n",
      "346\n",
      "Sentiment(polarity=0.23481188287368063, subjectivity=0.4237932760123771)\n",
      "347\n",
      "Sentiment(polarity=0.2225543478260869, subjectivity=0.5179330633134981)\n",
      "348\n",
      "Sentiment(polarity=0.22268170426065162, subjectivity=0.41773182957393484)\n",
      "349\n",
      "Sentiment(polarity=0.16309211974466217, subjectivity=0.47663474943135975)\n",
      "350\n",
      "Sentiment(polarity=0.10578162578162577, subjectivity=0.423663620330287)\n",
      "351\n",
      "Sentiment(polarity=0.11279884887839436, subjectivity=0.47726780794962614)\n",
      "352\n",
      "Sentiment(polarity=0.31969696969696965, subjectivity=0.5742424242424242)\n",
      "353\n",
      "Sentiment(polarity=0.19066558441558443, subjectivity=0.5338406385281385)\n",
      "354\n",
      "Sentiment(polarity=0.12895135566188198, subjectivity=0.39836124401913875)\n",
      "355\n",
      "Sentiment(polarity=0.16760789715335167, subjectivity=0.4856650924832744)\n",
      "356\n",
      "Sentiment(polarity=0.11287643515904387, subjectivity=0.4057923960097874)\n",
      "357\n",
      "Sentiment(polarity=0.15617882117882115, subjectivity=0.44951548451548445)\n",
      "358\n",
      "Sentiment(polarity=0.17449945887445886, subjectivity=0.44986020923520925)\n",
      "359\n",
      "Sentiment(polarity=0.15, subjectivity=0.43409090909090914)\n",
      "360\n",
      "Sentiment(polarity=0.22898550724637676, subjectivity=0.5719367588932807)\n",
      "361\n",
      "Sentiment(polarity=0.22255092130092136, subjectivity=0.47606920856920854)\n",
      "362\n",
      "Sentiment(polarity=0.21944444444444444, subjectivity=0.37777777777777777)\n",
      "363\n",
      "Sentiment(polarity=0.037500000000000006, subjectivity=0.2750739644970414)\n",
      "364\n",
      "Sentiment(polarity=0.09823232323232323, subjectivity=0.3785682916117699)\n",
      "365\n",
      "Sentiment(polarity=0.18821548821548825, subjectivity=0.45661922252831344)\n",
      "366\n",
      "Sentiment(polarity=0.1693335830835831, subjectivity=0.5044655344655344)\n",
      "367\n",
      "Sentiment(polarity=0.15977272727272726, subjectivity=0.5759090909090908)\n",
      "368\n",
      "Sentiment(polarity=0.1390630797773655, subjectivity=0.400448361162647)\n",
      "369\n",
      "Sentiment(polarity=0.10637376807589571, subjectivity=0.3624481901077646)\n",
      "370\n",
      "Sentiment(polarity=0.2845238095238096, subjectivity=0.47087301587301583)\n",
      "371\n",
      "Sentiment(polarity=0.1446078328359882, subjectivity=0.43052613513778565)\n",
      "372\n",
      "Sentiment(polarity=0.12291756854256854, subjectivity=0.4654279401154402)\n",
      "373\n",
      "Sentiment(polarity=0.29007177033492826, subjectivity=0.4625199362041467)\n",
      "374\n",
      "Sentiment(polarity=0.21063054009482576, subjectivity=0.45265924551638836)\n",
      "375\n",
      "Sentiment(polarity=0.18214285714285708, subjectivity=0.45615079365079375)\n",
      "376\n",
      "Sentiment(polarity=0.19458585858585858, subjectivity=0.38145550745550744)\n",
      "377\n",
      "Sentiment(polarity=-0.01428571428571429, subjectivity=0.31587301587301586)\n",
      "378\n",
      "Sentiment(polarity=0.21030126336248786, subjectivity=0.5064868804664725)\n",
      "379\n",
      "Sentiment(polarity=0.20021825396825396, subjectivity=0.49918650793650793)\n",
      "380\n",
      "Sentiment(polarity=0.10352586010480751, subjectivity=0.522582023239918)\n",
      "381\n",
      "Sentiment(polarity=0.1668648018648019, subjectivity=0.42989121989121987)\n",
      "382\n",
      "Sentiment(polarity=0.20462070874861568, subjectivity=0.4455177187153931)\n",
      "383\n",
      "Sentiment(polarity=0.1430555555555556, subjectivity=0.3784722222222222)\n",
      "384\n",
      "Sentiment(polarity=-0.004166666666666661, subjectivity=0.3583333333333334)\n",
      "385\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "386\n",
      "Sentiment(polarity=0.2055295693007557, subjectivity=0.4591367671876145)\n",
      "387\n",
      "Sentiment(polarity=0.17391082087104814, subjectivity=0.42363521579430663)\n",
      "388\n",
      "Sentiment(polarity=0.13710437710437712, subjectivity=0.44137085137085136)\n",
      "389\n",
      "Sentiment(polarity=0.13838744588744586, subjectivity=0.3737492269635127)\n",
      "390\n",
      "Sentiment(polarity=0.1638751352813853, subjectivity=0.49113140331890337)\n",
      "391\n",
      "Sentiment(polarity=-0.1, subjectivity=0.6)\n",
      "392\n",
      "Sentiment(polarity=0.2012042502951594, subjectivity=0.49732585596221973)\n",
      "393\n",
      "Sentiment(polarity=0.09596049783549784, subjectivity=0.4780925324675324)\n",
      "394\n",
      "Sentiment(polarity=0.11351748645055733, subjectivity=0.39870783197554854)\n",
      "395\n",
      "Sentiment(polarity=0.11507142857142857, subjectivity=0.404142857142857)\n",
      "396\n",
      "Sentiment(polarity=0.14636752136752137, subjectivity=0.3761294261294261)\n",
      "397\n",
      "Sentiment(polarity=0.15452651515151516, subjectivity=0.45376893939393953)\n",
      "398\n",
      "Sentiment(polarity=0.2006177156177156, subjectivity=0.47496503496503506)\n",
      "399\n",
      "Sentiment(polarity=0.24314574314574317, subjectivity=0.5268879268879268)\n",
      "400\n",
      "Sentiment(polarity=0.1858075258075258, subjectivity=0.56002997002997)\n",
      "401\n",
      "Sentiment(polarity=0.1628479672501412, subjectivity=0.4862168266516093)\n",
      "402\n",
      "Sentiment(polarity=0.2076627384960718, subjectivity=0.5098737373737376)\n",
      "403\n",
      "Sentiment(polarity=0.20679048892284185, subjectivity=0.42491787624140565)\n",
      "404\n",
      "Sentiment(polarity=0.1274851190476191, subjectivity=0.49543154761904773)\n",
      "405\n",
      "Sentiment(polarity=0.17932162534435264, subjectivity=0.4918044077134987)\n",
      "406\n",
      "Sentiment(polarity=0.3194444444444444, subjectivity=0.522718253968254)\n",
      "407\n",
      "Sentiment(polarity=0.14232954545454546, subjectivity=0.47945075757575756)\n",
      "408\n",
      "Sentiment(polarity=0.12910695626212865, subjectivity=0.3613804299149126)\n",
      "409\n",
      "Sentiment(polarity=0.1910561482168625, subjectivity=0.49694774273345693)\n",
      "410\n",
      "Sentiment(polarity=0.1692067736185383, subjectivity=0.4469474153297683)\n",
      "411\n",
      "Sentiment(polarity=0.3325962325962326, subjectivity=0.4688943488943488)\n",
      "412\n",
      "Sentiment(polarity=0.14082505729564554, subjectivity=0.4042491495432671)\n",
      "413\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "414\n",
      "Sentiment(polarity=0.054958771387342814, subjectivity=0.38882086167800445)\n",
      "415\n",
      "Sentiment(polarity=0.24314574314574317, subjectivity=0.5268879268879268)\n",
      "416\n",
      "Sentiment(polarity=0.14583333333333334, subjectivity=0.3902777777777778)\n",
      "417\n",
      "Sentiment(polarity=0.11923076923076925, subjectivity=0.356753663003663)\n",
      "418\n",
      "Sentiment(polarity=0.18857608225108227, subjectivity=0.5321766233766233)\n",
      "419\n",
      "Sentiment(polarity=0.11484576030030574, subjectivity=0.38434822753004566)\n",
      "420\n",
      "Sentiment(polarity=0.1397453196154495, subjectivity=0.44752768876145504)\n",
      "421\n",
      "Sentiment(polarity=0.2006177156177156, subjectivity=0.47496503496503506)\n",
      "422\n",
      "Sentiment(polarity=0.17558380298946333, subjectivity=0.5125053091562526)\n",
      "423\n",
      "Sentiment(polarity=0.2074945887445887, subjectivity=0.5004058441558441)\n",
      "424\n",
      "Sentiment(polarity=0.1638751352813853, subjectivity=0.49113140331890337)\n",
      "425\n",
      "Sentiment(polarity=0.18363320707070707, subjectivity=0.4555961399711399)\n",
      "426\n",
      "Sentiment(polarity=0.19267550017550023, subjectivity=0.5319047619047619)\n",
      "427\n",
      "Sentiment(polarity=0.10947089947089944, subjectivity=0.3573611111111112)\n",
      "428\n",
      "Sentiment(polarity=0.21522702104097452, subjectivity=0.5033222591362126)\n",
      "429\n",
      "Sentiment(polarity=0.11797440002919457, subjectivity=0.42083108672149766)\n",
      "430\n",
      "Sentiment(polarity=0.20981162981162982, subjectivity=0.5471498771498771)\n",
      "431\n",
      "Sentiment(polarity=0.31283643892339547, subjectivity=0.4187888198757764)\n",
      "432\n",
      "Sentiment(polarity=0.11785714285714285, subjectivity=0.46071428571428574)\n",
      "433\n",
      "Sentiment(polarity=0.10038522984951556, subjectivity=0.32083719851577)\n",
      "434\n",
      "Sentiment(polarity=0.18437872937872937, subjectivity=0.45885573885573877)\n",
      "435\n",
      "Sentiment(polarity=0.17939986600700888, subjectivity=0.4869923727066583)\n",
      "436\n",
      "Sentiment(polarity=0.06888359036796539, subjectivity=0.5314410849567099)\n",
      "437\n",
      "Sentiment(polarity=0.29124999999999995, subjectivity=0.5808333333333333)\n",
      "438\n",
      "Sentiment(polarity=0.11406926406926406, subjectivity=0.4899008885850991)\n",
      "439\n",
      "Sentiment(polarity=0.19462473572938685, subjectivity=0.42512508809020433)\n",
      "440\n",
      "Sentiment(polarity=0.18015873015873013, subjectivity=0.5257936507936508)\n",
      "441\n",
      "Sentiment(polarity=0.20829857277225705, subjectivity=0.5194019430861535)\n",
      "442\n",
      "Sentiment(polarity=0.22927182880013067, subjectivity=0.48825859674916283)\n",
      "443\n",
      "Sentiment(polarity=0.17127845273006567, subjectivity=0.4239107666527021)\n",
      "444\n",
      "Sentiment(polarity=0.032348748812163444, subjectivity=0.3134040755991976)\n",
      "445\n",
      "Sentiment(polarity=0.19569597069597072, subjectivity=0.4028846153846154)\n",
      "446\n",
      "Sentiment(polarity=0.1960767590618337, subjectivity=0.5091791044776119)\n",
      "447\n",
      "Sentiment(polarity=0.21197916666666666, subjectivity=0.6135416666666667)\n",
      "448\n",
      "Sentiment(polarity=0.06857413419913422, subjectivity=0.2990530303030303)\n",
      "449\n",
      "Sentiment(polarity=0.12321428571428572, subjectivity=0.45119047619047625)\n",
      "450\n",
      "Sentiment(polarity=0.15035714285714288, subjectivity=0.4913492063492064)\n",
      "451\n",
      "Sentiment(polarity=0.012499999999999983, subjectivity=0.4770833333333334)\n",
      "452\n",
      "Sentiment(polarity=0.1390630797773655, subjectivity=0.400448361162647)\n",
      "453\n",
      "Sentiment(polarity=0.10637376807589571, subjectivity=0.3624481901077646)\n",
      "454\n",
      "Sentiment(polarity=0.2845238095238096, subjectivity=0.47087301587301583)\n",
      "455\n",
      "Sentiment(polarity=0.12291756854256854, subjectivity=0.4654279401154402)\n",
      "456\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "457\n",
      "Sentiment(polarity=0.1446078328359882, subjectivity=0.43052613513778565)\n",
      "458\n",
      "Sentiment(polarity=0.3143356643356643, subjectivity=0.5497086247086247)\n",
      "459\n",
      "Sentiment(polarity=0.18214285714285708, subjectivity=0.45615079365079375)\n",
      "460\n",
      "Sentiment(polarity=0.1430555555555556, subjectivity=0.3784722222222222)\n",
      "461\n",
      "Sentiment(polarity=0.21063054009482576, subjectivity=0.45265924551638836)\n",
      "462\n",
      "Sentiment(polarity=0.19458585858585858, subjectivity=0.38145550745550744)\n",
      "463\n",
      "Sentiment(polarity=0.20021825396825396, subjectivity=0.49918650793650793)\n",
      "464\n",
      "Sentiment(polarity=0.21030126336248786, subjectivity=0.5064868804664725)\n",
      "465\n",
      "Sentiment(polarity=-0.01428571428571429, subjectivity=0.31587301587301586)\n",
      "466\n",
      "Sentiment(polarity=-0.004166666666666661, subjectivity=0.3583333333333334)\n",
      "467\n",
      "Sentiment(polarity=0.10352586010480751, subjectivity=0.522582023239918)\n",
      "468\n",
      "Sentiment(polarity=0.29007177033492826, subjectivity=0.4625199362041467)\n",
      "469\n",
      "Sentiment(polarity=0.1285064935064935, subjectivity=0.29988559059987635)\n",
      "470\n",
      "Sentiment(polarity=0.2055295693007557, subjectivity=0.4591367671876145)\n",
      "471\n",
      "Sentiment(polarity=0.1638751352813853, subjectivity=0.49113140331890337)\n",
      "472\n",
      "Sentiment(polarity=0.13838744588744586, subjectivity=0.3737492269635127)\n",
      "473\n",
      "Sentiment(polarity=0.09596049783549784, subjectivity=0.4780925324675324)\n",
      "474\n",
      "Sentiment(polarity=0.20462070874861568, subjectivity=0.4455177187153931)\n",
      "475\n",
      "Sentiment(polarity=0.11507142857142857, subjectivity=0.404142857142857)\n",
      "476\n",
      "Sentiment(polarity=0.2528535353535354, subjectivity=0.568181818181818)\n",
      "477\n",
      "Sentiment(polarity=0.13710437710437712, subjectivity=0.44137085137085136)\n",
      "478\n",
      "Sentiment(polarity=0.1668648018648019, subjectivity=0.42989121989121987)\n",
      "479\n",
      "Sentiment(polarity=0.17391082087104814, subjectivity=0.42363521579430663)\n",
      "480\n",
      "Sentiment(polarity=0.11385088126159555, subjectivity=0.38082096474953625)\n",
      "481\n",
      "Sentiment(polarity=0.1346699134199134, subjectivity=0.41154852092352084)\n",
      "482\n",
      "Sentiment(polarity=0.24314574314574317, subjectivity=0.5268879268879268)\n",
      "483\n",
      "Sentiment(polarity=0.054958771387342814, subjectivity=0.38882086167800445)\n",
      "484\n",
      "Sentiment(polarity=0.14082505729564554, subjectivity=0.4042491495432671)\n",
      "485\n",
      "Sentiment(polarity=0.11385088126159555, subjectivity=0.38082096474953625)\n",
      "486\n",
      "Sentiment(polarity=0.07118235930735929, subjectivity=0.3190503246753246)\n",
      "487\n",
      "Sentiment(polarity=0.17777777777777778, subjectivity=0.41111111111111115)\n",
      "488\n",
      "Sentiment(polarity=0.18513419913419912, subjectivity=0.46412987012987006)\n",
      "489\n",
      "Sentiment(polarity=0.21491228070175442, subjectivity=0.5741228070175438)\n",
      "490\n",
      "Sentiment(polarity=0.2132525420316118, subjectivity=0.46444553508506997)\n",
      "491\n",
      "Sentiment(polarity=0.1298079004329004, subjectivity=0.4520664751914751)\n",
      "492\n",
      "Sentiment(polarity=0.14910511363636364, subjectivity=0.4347253787878787)\n",
      "493\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "494\n",
      "Sentiment(polarity=0.1720839345839346, subjectivity=0.5106361231361232)\n",
      "495\n",
      "Sentiment(polarity=0.2238992869875223, subjectivity=0.4546806298276887)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df2)):\n",
    "    print(i)\n",
    "    print(TextBlob(df2.job_posting[i]).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Greetings,\\nHope you doing good.\\nThis is Karan Senior Technical Recruiter form STCG Inc.. We have come across your profile on LinkedIn/Job Boards and we found your profile matching to the below job description.\\nJob Title: Data Scientist\\nLocation : Redmond, WA & Palo Alto, CA\\nDuration : Long term Contract\\nThis is junior to mid level position ( look for around 3 years of PHD guys)\\nKey skills needed:\\nData Science and Machine learning techniques.\\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.\\nHands on experience with ML techniques for text processing\\nHand on experience with Deep learning techniques (Not mandatory)\\nProgramming with R or Python, SQL\\nJob Type: Contract\\nRequired experience:\\nTotal IT: 10 years\\nMachine Learing: 6 years\\nScripting: 7 years\\nData Scientist: 8 years\\nRequired education:\\nBachelor's\\nJob Location:\\nRedmond, WA\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.job_posting[194]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            and 44\n",
      "             to 30\n",
      "             of 16\n",
      "            the 15\n",
      "         models 13\n",
      "              a 11\n",
      "           data 10\n",
      "           with 10\n",
      "           that 10\n",
      "     experience 10\n",
      "             as 10\n",
      "             we 9\n",
      "            our 9\n",
      "             at 8\n",
      "            for 7\n",
      "            are 6\n",
      "             is 6\n",
      "             or 6\n",
      "          build 5\n",
      "              ’ 5\n",
      "           team 5\n",
      "             in 5\n",
      "        science 4\n",
      "          scale 4\n",
      "       identify 4\n",
      "             an 4\n",
      "          teams 3\n",
      "            one 3\n",
      "        serving 3\n",
      "    maintaining 3\n",
      "           they 3\n",
      "           what 3\n",
      "           them 3\n",
      "          asset 3\n",
      "       bluecore 3\n",
      "          their 3\n",
      "      customers 3\n",
      "          model 3\n",
      "           such 3\n",
      "       leverage 3\n",
      "       relevant 3\n",
      "       learning 3\n",
      "            all 3\n",
      "     scientists 2\n",
      "         strong 2\n",
      "    backgrounds 2\n",
      "           work 2\n",
      "    engineering 2\n",
      "        empower 2\n",
      "      marketers 2\n",
      "          years 2\n",
      "       building 2\n",
      "           have 2\n",
      "          eager 2\n",
      "          share 2\n",
      "        members 2\n",
      "        through 2\n",
      "        process 2\n",
      "     accelerate 2\n",
      "             on 2\n",
      "        finally 2\n",
      "             be 2\n",
      "        explore 2\n",
      "          other 2\n",
      "        working 2\n",
      "      exploring 2\n",
      "             re 2\n",
      "       powerful 2\n",
      "           make 2\n",
      "      decisions 2\n",
      "           most 2\n",
      "       possible 2\n",
      "       customer 2\n",
      "              s 2\n",
      "        product 2\n",
      "       flexible 2\n",
      "        diverse 2\n",
      "           from 2\n",
      "            sql 2\n",
      "         google 2\n",
      "         engine 2\n",
      "        applied 2\n",
      "           into 2\n",
      "         neural 2\n",
      "       networks 2\n",
      "        propose 2\n",
      "   requirements 2\n",
      "     statistics 2\n",
      "       industry 2\n",
      "   optimization 2\n",
      "           deep 2\n",
      "      including 2\n",
      "    opportunity 2\n",
      "        monthly 2\n",
      "        fitness 2\n",
      "    environment 2\n",
      "         gender 2\n",
      "         status 2\n",
      "        looking 1\n",
      "         senior 1\n",
      "   mathematical 1\n",
      "      alongside 1\n",
      "           next 1\n",
      "     generation 1\n",
      "         retail 1\n",
      "       commerce 1\n",
      "        delight 1\n",
      "          ideal 1\n",
      "      candidate 1\n",
      "            has 1\n",
      "        several 1\n",
      "    researching 1\n",
      "     first-hand 1\n",
      "          works 1\n",
      "          doesn 1\n",
      "              t 1\n",
      "           this 1\n",
      "           more 1\n",
      "         junior 1\n",
      "          guide 1\n",
      "           also 1\n",
      "           able 1\n",
      "        excited 1\n",
      "           help 1\n",
      "      architect 1\n",
      "            out 1\n",
      "   architecture 1\n",
      "         needed 1\n",
      "     innovation 1\n",
      "     facilitate 1\n",
      "         should 1\n",
      "        curious 1\n",
      "         myriad 1\n",
      "       products 1\n",
      "            can 1\n",
      "          built 1\n",
      "        culture 1\n",
      "     emphasizes 1\n",
      "         making 1\n",
      "           good 1\n",
      "      tradeoffs 1\n",
      "        leaving 1\n",
      "           your 1\n",
      "            ego 1\n",
      "           door 1\n",
      "    first-party 1\n",
      "           core 1\n",
      "     everything 1\n",
      "      forefront 1\n",
      "     innovative 1\n",
      "       exciting 1\n",
      "           ways 1\n",
      "       activate 1\n",
      "      dedicated 1\n",
      "      engineers 1\n",
      "       together 1\n",
      "          right 1\n",
      "         engage 1\n",
      "   personalized 1\n",
      "        content 1\n",
      "       approach 1\n",
      "       academic 1\n",
      "       starting 1\n",
      "     literature 1\n",
      "         search 1\n",
      "       baseline 1\n",
      "      iterative 1\n",
      "       training 1\n",
      "     validation 1\n",
      "       suitable 1\n",
      "         simple 1\n",
      "      necessary 1\n",
      "         employ 1\n",
      "           wide 1\n",
      "        variety 1\n",
      "       bayesian 1\n",
      "     predicting 1\n",
      "       lifetime 1\n",
      "          value 1\n",
      "         matrix 1\n",
      "  factorization 1\n",
      "       affinity 1\n",
      "        operate 1\n",
      "         crunch 1\n",
      "       millions 1\n",
      "         points 1\n",
      "           been 1\n",
      "          shown 1\n",
      "         double 1\n",
      "        revenue 1\n",
      "         triple 1\n",
      "          reach 1\n",
      "       designed 1\n",
      "         manner 1\n",
      "     generalize 1\n",
      "         across 1\n",
      "            set 1\n",
      "            300 1\n",
      "            who 1\n",
      "           span 1\n",
      "     industries 1\n",
      "        apparel 1\n",
      "     automotive 1\n",
      "         deploy 1\n",
      "       maintain 1\n",
      "           many 1\n",
      "          tools 1\n",
      "       bigquery 1\n",
      "          spark 1\n",
      "          cloud 1\n",
      "          keras 1\n",
      "     tensorflow 1\n",
      "        airflow 1\n",
      "            app 1\n",
      "        compute 1\n",
      "         values 1\n",
      "       research 1\n",
      "     constantly 1\n",
      "       frontier 1\n",
      "         diving 1\n",
      "         topics 1\n",
      "          topic 1\n",
      "       modeling 1\n",
      "     restricted 1\n",
      "      boltzmann 1\n",
      "       machines 1\n",
      "      recurrent 1\n",
      "  convolutional 1\n",
      "          image 1\n",
      "        feature 1\n",
      "     extraction 1\n",
      "       recently 1\n",
      "      extensive 1\n",
      "           dive 1\n",
      "   differential 1\n",
      "        privacy 1\n",
      "responsibilities 1\n",
      "            new 1\n",
      "  opportunities 1\n",
      "          drive 1\n",
      "      technical 1\n",
      "    initiatives 1\n",
      " infrastructure 1\n",
      "           pace 1\n",
      "    exploration 1\n",
      "        improve 1\n",
      "    maintenance 1\n",
      "           meet 1\n",
      "        discuss 1\n",
      "        roadmap 1\n",
      "    identifying 1\n",
      "    appropriate 1\n",
      "models/algorithms 1\n",
      "          solve 1\n",
      "     meticulous 1\n",
      "experimentation 1\n",
      "       evaluate 1\n",
      "        compare 1\n",
      "        writing 1\n",
      "       internal 1\n",
      "       external 1\n",
      "         facing 1\n",
      "  documentation 1\n",
      "     describing 1\n",
      "     approaches 1\n",
      "      deploying 1\n",
      "     production 1\n",
      "            phd 1\n",
      "             ms 1\n",
      "       computer 1\n",
      "     electrical 1\n",
      "           math 1\n",
      "   quantitative 1\n",
      "    disciplines 1\n",
      "     equivalent 1\n",
      "     coursework 1\n",
      "         fields 1\n",
      "        machine 1\n",
      "  understanding 1\n",
      "statistical/probabilistic 1\n",
      "       analysis 1\n",
      "         linear 1\n",
      "        algebra 1\n",
      "              3 1\n",
      "    internships 1\n",
      "        ability 1\n",
      "          write 1\n",
      "production-ready 1\n",
      "           code 1\n",
      "        natural 1\n",
      "       language 1\n",
      "     processing 1\n",
      "  reinforcement 1\n",
      "         and/or 1\n",
      "             ml 1\n",
      "      computing 1\n",
      "hadoop/scala/spark/cassandra 1\n",
      "       benefits 1\n",
      "         highly 1\n",
      "    competitive 1\n",
      "   compensation 1\n",
      "        package 1\n",
      "         salary 1\n",
      "         equity 1\n",
      "           well 1\n",
      "        fastest 1\n",
      "        growing 1\n",
      "      marketing 1\n",
      "     technology 1\n",
      "      start-ups 1\n",
      "  comprehensive 1\n",
      "        medical 1\n",
      "         dental 1\n",
      "         vision 1\n",
      "      insurance 1\n",
      "            401 1\n",
      "              k 1\n",
      "           plan 1\n",
      "        stipend 1\n",
      "            gym 1\n",
      "     membership 1\n",
      "        classes 1\n",
      "            nyc 1\n",
      "          metro 1\n",
      "           card 1\n",
      "       generous 1\n",
      "       parental 1\n",
      "          leave 1\n",
      "       vacation 1\n",
      "         policy 1\n",
      "        believe 1\n",
      "      fostering 1\n",
      "      inclusive 1\n",
      "          which 1\n",
      "      employees 1\n",
      "           feel 1\n",
      "     encouraged 1\n",
      "         unique 1\n",
      "   perspectives 1\n",
      "      strengths 1\n",
      "            act 1\n",
      "  authentically 1\n",
      "           know 1\n",
      "        welcome 1\n",
      "          those 1\n",
      "        varying 1\n",
      "    experiences 1\n",
      "          proud 1\n",
      "          equal 1\n",
      "       employer 1\n",
      "      committed 1\n",
      "           fair 1\n",
      "         hiring 1\n",
      "      practices 1\n",
      "       creating 1\n",
      "      welcoming 1\n",
      "      qualified 1\n",
      "     applicants 1\n",
      "           will 1\n",
      "        receive 1\n",
      "  consideration 1\n",
      "     employment 1\n",
      "        without 1\n",
      "         regard 1\n",
      "           race 1\n",
      "          color 1\n",
      "       religion 1\n",
      "       identity 1\n",
      "     expression 1\n",
      "         sexual 1\n",
      "    orientation 1\n",
      "       national 1\n",
      "         origin 1\n",
      "     disability 1\n",
      "            age 1\n",
      "       familial 1\n",
      "        veteran 1\n"
     ]
    }
   ],
   "source": [
    "def get_count(item):\n",
    "    return item[1]\n",
    "\n",
    "for word, count in sorted(jobpost_blob.word_counts.items(), key=get_count, reverse=True):\n",
    "    print(\"%15s %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    computer science 296\n",
      "communication skills 149\n",
      "  sexual orientation 140\n",
      "     national origin 137\n",
      "   equal opportunity 124\n",
      "      without regard 117\n",
      "      veteran status 117\n",
      "       related field 114\n",
      "          race color 109\n",
      "         regard race 108\n",
      "            new york 108\n",
      "qualified applicants 98\n",
      "     gender identity 97\n",
      "        ability work 93\n",
      "        science team 92\n",
      "   business problems 89\n",
      "            r python 88\n",
      "      skills ability 86\n",
      "      color religion 84\n",
      " predictive modeling 82\n",
      "            job type 82\n",
      "        team members 81\n",
      "         bachelor 's 81\n",
      "software development 78\n",
      "  orientation gender 77\n",
      "            master ’ 75\n",
      " operations research 73\n",
      "          large sets 72\n",
      "            python r 71\n",
      "          bachelor ’ 69\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"']\n",
    "stop += ['degree', 'phd', 'employer', 'data', \"analytics\", 'analysis', 'machine', 'learning', 'experience', 'years']\n",
    "stop = set(stop)\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "n = 2\n",
    "for doc in df2.job_posting:\n",
    "    doc = doc.lower()\n",
    "    words = TextBlob(doc).words  # tokenize words\n",
    "    words = [w for w in words if w not in stop]   \n",
    "    bigrams = ngrams(words, n)\n",
    "    counter += Counter(bigrams)\n",
    "\n",
    "for phrase, count in counter.most_common(30):\n",
    "    print('%20s %i' % (\" \".join(phrase), count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TF: frequency in this document\n",
    "#### IDF: inverse frequency in the corpus\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
    "#doc_vectors = vectorizer.fit_transform(df.job_posting)\n",
    "#classes = np.array(['pos']*int(len(df)/2) + ['neg']*int(len(df)/2))\n",
    "#model = MultinomialNB().fit(doc_vectors, classes)\n",
    "\n",
    "#job_post_vector = vectorizer.transform(df.job_posting)\n",
    "#model.predict(job_post_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# testing with job posts as docs\n",
    "documents = df2.job_posting\n",
    "\n",
    "no_features = 500\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.7, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_topics = 16\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "management support project development research systems software information solutions technical quality provide design related including internal analysis ability requirements projects clinical ensure knowledge develop intelligence integration processes process environment tools demonstrated working implementation needs teams security stakeholders lead performance technology standards years database minimum relevant required technologies meet practices application functional federal scientific level manage reporting based expertise provides external effectively open communication activities visualization strategies applications source problem leadership maintain qualifications specific review discipline new concepts programs end opportunities operational plans training critical position responsibilities candidates analytic organization change department delivery computer high preferred innovation excellent written best manager\n",
      "Topic 1:\n",
      "learning machine techniques models algorithms scientist python using analysis statistical strong predictive deep statistics problems modeling large research building datasets looking mining years advanced tools develop new programming understanding methods computer world processing regression software company join knowledge sql model scientists mathematics languages time ability use phd language good optimization engineering real quantitative applying job candidate insights working build qualifications solve spark production financial mathematical approaches analytics applied analytic field preferred apply technologies results communication environment responsibilities technical related testing projects including developing solutions hands help complex sets java industry required background clustering engineers natural systems creating better degree description\n",
      "Topic 2:\n",
      "status employment gender protected applicants race national color origin disability religion opportunity sexual age equal orientation veteran qualified employer sex regard identity demonstrated analytics law marital expression consideration receive methods committed information modeling applicable individuals job communicate digital employees decisions engineering projects diverse results action use diversity value test scientist solutions basis qualifications position future security role program degree providing industry development existing statistical need drive learn better questions master multiple analyses software organization cross users deployment company implement engineers industrial employee technical problems proficiency grow professional make include optimize based opportunities tools offer culture media address federal background fast\n",
      "Topic 3:\n",
      "people sales world marketing communities grow advertising teams small helping build clients mission global community ways bring working platforms leading building success help insights digital operations different customers new creating platform driven organization like problems partner businesses ideal managing leader support company directly customer cross ad dynamic quantitative qualifications products collaborative culture real field resources finance impact solving make engineering strategy services math office analytical consumer range media tools track best reports understand member 10 leadership unique diverse physics benefits technology travel passion service contribute minimum create statistics action enable class scientists record industry solutions user managers responsibilities position economics\n",
      "Topic 4:\n",
      "firm investment group candidates partners financial services results insights information model strong demonstrated methodologies technology ability qualified using www analytical questions create com analyze proven manage clients projects end drive management businesses portfolio modeling help excel techniques corporate resume infrastructure role learn communication position overview goals professionals growth analyst issues knowledge strategic environment change successful managers focused expected test following generate required provide levels offer self unique wide presentation company order learning senior evaluate individuals highly present paced various available finance internal manner programming training year audiences summary languages deliver excellent solutions verbal platforms global making closely level used stakeholders\n",
      "Topic 5:\n",
      "analytics global service predictive strategic leadership sciences value customers growth advanced success commercial position modeling market impact portfolio significant industrial leading collaborate operations contribute vision developing analytical individual drive plus services knowledge metrics bachelor director define leaders participate teams professional technology development ability decision trends application ms life high capabilities new responsible making technical role job lead methods platform leverage analyst focus execution familiarity translate relationships demonstrated solutions type delivery area questions analysts practices plans effectively machine degree statistics presentation expected primary insights engineering working plan operational marketing self use senior learning different industry reports statistical systems looking areas existing\n",
      "Topic 6:\n",
      "hadoop big growing content plus systems production web database spark design scale distributed build real scalable algorithms scala mining hive global required applications various deliver years code users engineering cloud time java platform tech requirements bachelor predictive sql machine small companies online enable collaborative learning initiatives product implementation actionable new audiences applicable great implementing maintain implement recommendations optimization tools findings related office drive large present enterprise based way role communication teams sources non variety people statistics analytics model com python development degree services products technology customer competitive high candidates quantitative field modeling advertising identify technical programming infrastructure processing support available\n",
      "Topic 7:\n",
      "capgemini client group practice management needs collaborative global present digital services specific regression resources solutions www findings statistical application model media delivery testing way learn analysis demonstrate sciences year status building com technology following equal big knowledge scalable innovation networks preferred generate consulting distributed expertise law expression applications life marital employment description capabilities 000 systems functional languages algorithms leader working provide learning technical understanding analytics achieve solving engineering diversity understand growing position machine design products agile environment receive structured methods regard perform problems consideration solution decision classification able series identity manner duties overview models matlab goals web people opportunity consumer\n",
      "Topic 8:\n",
      "customer sets large analytical perform functions like analyze database marketing duties managing oriented position sources tasks analyst individual reporting responsible analysis projects protected education decisions job reports ability questions manage equivalent required summary ensure web ad insights current ideal maintain field candidate analytics hive spark understand qualifications improve value effectively sql financial years strategy growing applicable basis support organization define demonstrate areas complex statistical include proficiency subject develop excellent providing degree customers related needs committed excel assist developing responsibilities preferred expected company functional description high seeking expression technical equal able results tools strong actionable analyses design employees marital individuals math\n",
      "Topic 9:\n",
      "health care healthcare medical clinical america solutions education plan complex analysis agency individual requirements analytics products analytic companies digital including assist position organization access services delivery lead advanced include insights benefits partners life enterprise statistical marketing preferred providing knowledge innovative communities developing united analyses scientific future end outcomes equivalent analyze applications address perform opportunity diversity creating quality analytical provide effective related vision tools duties physics years states people company tableau level design members communications external demonstrated quantitative visualization focused impact variety degree execution needs opportunities presentations primary sql wide user exceptional insurance results strong background required approaches code contribute projects\n",
      "Topic 10:\n",
      "job required 000 education master type years salary year location support scientific time 10 desired language subject program minimum presentations include tasks level range highly expert operations degree bachelor position resume management products development field writing research york engineering advertising knowledge related tableau economics statistics good application platform use python duties summary programming new manager able requirements web test techniques assist computer insurance developing service benefits visualization preferred matlab applicants programs phd present needed need decision computational client learning scientists strong candidates leaders using media artificial information initiatives investment employee state com travel engineers employer effectively translate effective employees trends\n",
      "Topic 11:\n",
      "reliability engineering quality machine analysis resources physics discipline focused analytics providing cross employees related future database focus bachelor functional knowledge statistical processing product resume expertise opportunity mathematics demonstrate learning use information enterprise employee class strong degree career type passionate companies computer statistics current relationships develop available meet matlab duties similar service familiarity systems processes working sas track need health process client tools databases equal innovative performance education needs model datasets world leadership analyze apply closely job years mining methods clients position intelligence understanding provide advanced technology building services preferred teams required requirements software ability programming sql modeling solutions large support\n",
      "Topic 12:\n",
      "ll engineering analytics make decision analysis help professionals analyst bring communications solving ad making driven intelligence decisions professional improve problem visualization able best people databases create world strong organization candidate technical languages quality project large operations provide programming working solutions statistical scientist technology ability computer better background looking need analysts managers relevant engineers scientists java computing performance proficiency client python directly day businesses high testing program problems natural equivalent new year mathematics change platform hands customers tech growth datasets passion based office understand investment excel excellent diversity exceptional evaluate diverse drive duties dynamic enable economics education equal employment enterprise ensure\n",
      "Topic 13:\n",
      "product products metrics quantitative user company teams key success com understanding new drive decisions digital analyses working ability help results engineering marketing www fast consumer insights york leadership trends users based customer closely analysis sets platform self role development understand writing analytics partner strategy opportunities manager looking define making technical production presentation passionate sql build better building field present growth market make growing goals driven leaders learn highly scientists identify improve paced offer years statistics deliver hive actionable mission best people core preferred problems requirements cross location high vision levels communicate impact analysts math ms hadoop infrastructure responsibilities solve partners\n",
      "Topic 14:\n",
      "risk financial state management including quantitative companies finance analysis models testing security investment areas individual large based analyst implementing information clients operational end operations reporting open requirements source ensure ability corporate developing modeling market leading sources area existing com using tools visualization analytical diverse specific day consumer insurance background natural sets methodologies identify intelligence gender sql capabilities artificial primary related culture working platforms making processing organizations analyze responsible commercial datasets firm database small implement mathematics support 000 class time integration meet effectively sexual conduct review classification recommendations achieve analytic expert expertise expression external familiarity fast federal analysts field analyses america\n",
      "Topic 15:\n",
      "clients analytics insurance consulting client advanced new commercial value big help healthcare practices core models expertise industry develop media impact network processes identify dynamic tech years opportunities firm strategies innovative growing sets management available leading york marketing strategic relevant current organizations methodologies seeking practice innovation key improve capabilities role plus range development findings leadership high deliver growth sas degree tableau presentations focus social public manager thinking services life analytical including make join ll insights appropriate actionable statistical translate achieve multiple agency career successful office unstructured programs strategy creating ability use director issues provide external member needs recommendations predictive complex technology\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 100\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove equal opportunity clause and topic 10 stuff. Try adding more stopwords. Rerun the above.\n",
    "# try subsetting by job roles, job levels, region (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try LDA -- looks pretty useless here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LDA\n",
    "no_topics = 15\n",
    "lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "project management clinical provide required support change job experience team development education activities standards plans plan design minimum manager review\n",
      "Topic 1:\n",
      "team business including benefits management techniques company employees decision mining modeling present support quality value plan model algorithms office variety\n",
      "Topic 2:\n",
      "experience machine learning management change ability science team spark products years technical build skills services research algorithms business support work\n",
      "Topic 3:\n",
      "experience team solutions analysis work python business knowledge job engineering skills status employment years design hands related technology using position\n",
      "Topic 4:\n",
      "business analytics team science work experience clients client management services skills new advanced solutions ability analytical firm strategic technology learning\n",
      "Topic 5:\n",
      "experience science modeling world development machine skills learning building analysis python techniques algorithms years time new role scientist apply models\n",
      "Topic 6:\n",
      "capgemini business information development management technology analysis status working based federal services impact tools experience help expression people skills team\n",
      "Topic 7:\n",
      "business solutions team experience marketing company optimization customer people algorithms ad job large work world organization develop insights projects collaborative\n",
      "Topic 8:\n",
      "experience learning machine science years required techniques team skills research analytics models advanced business work python analysis statistics algorithms job\n",
      "Topic 9:\n",
      "customer experience large sets like analytical database business job protected team functions science managing marketing oriented analyst support tasks decisions\n",
      "Topic 10:\n",
      "experience learning science tools modeling statistical work analysis support statistics deep research qualifications medical related job software project phd preferred\n",
      "Topic 11:\n",
      "experience business work science team analysis skills development analytics engineering ability learning develop support solutions working new information tools technical\n",
      "Topic 12:\n",
      "business experience science digital learning knowledge team models information projects applicants analytics degree management customer solutions years new marketing problems\n",
      "Topic 13:\n",
      "models experience modeling analysis techniques develop including business skills analytical design projects strong statistics solutions field quantitative work large industry\n",
      "Topic 14:\n",
      "demonstrated requirements mining team science expertise predictive solutions industrial analytical industry firm modeling analytic understanding analytics work integration sets scientists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "no_top_words = 20\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At Bluecore we are transforming the way eCommerce marketers use data and automation to communicate with customers. Bluecore’s customer experience platform is designed to simplify the process of ingesting terabytes of behavioral data and automatically taking action on precise insights, driving engagement and conversion rates that defy industry standards. We're one of New York City’s fastest growing SaaS start-ups, working with more than 190 customers representing more than 250 high-end apparel, electronics, automotive and other consumer brands.\\r\\n\\r\\nWe have only one rule: use good judgment. We hire incredible people so we don’t have to institute cumbersome rules or processes. We want you to be free to think, create and get things done on your own terms. We love positive energy in the office. In fact, we need it—happy people are more efficient and creative. So if 6 PM improv classes or midday gym sessions are what makes you happy, we will support you. When you join us, you’re stuck with us. We hired you for you, not just for this job. We know the best people move up and often go on to start their own companies. We want to help you get where you want to go.\\r\\n\\r\\nPerks and Benefits\\r\\n- Highly competitive compensation package and opportunity to work for one of the fastest growing start-ups in New York City.\\r\\n- Laid-back office, casual attire. Jeans and a t-shirt always OK, though we have some pretty snappy dressers on this team. The choice is yours.\\r\\n- Free breakfast, lunch, snacks, drinks.\\r\\n- Ergonomic desk options from top to bottom.\\r\\n- Monthly fitness stipend. Yoga, CrossFit, Spin … your choice.\\r\\n- Monthly NYC metro card (or similar public transit pass) paid for.\\r\\n- You’ll receive a new MacBook Pro with appropriate software installed to complete your work.\\r\\n- Full medical, dental and vision benefits program.\\xa0–\\xa0\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.company_descr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Bluecore we are transforming the way eCommerce marketers use data and automation to communicate with customers. Bluecore’s customer experience platform is designed to simplify the process of ingesting terabytes of behavioral data and automatically taking action on precise insights, driving engagement and conversion rates that defy industry standards. We're one of New York City’s fastest growing SaaS start-ups, working with more than 190 customers representing more than 250 high-end apparel, electronics, automotive and other consumer brands.\r\n",
      "\r\n",
      "We have only one rule: use good judgment. We hire incredible people so we don’t have to institute cumbersome rules or processes. We want you to be free to think, create and get things done on your own terms. We love positive energy in the office. In fact, we need it—happy people are more efficient and creative. So if 6 PM improv classes or midday gym sessions are what makes you happy, we will support you. When you join us, you’re stuck with us. We hired you for you, not just for this job. We know the best people move up and often go on to start their own companies. We want to help you get where you want to go.\r\n",
      "\r\n",
      "Perks and Benefits\r\n",
      "- Highly competitive compensation package and opportunity to work for one of the fastest growing start-ups in New York City.\r\n",
      "- Laid-back office, casual attire. Jeans and a t-shirt always OK, though we have some pretty snappy dressers on this team. The choice is yours.\r\n",
      "- Free breakfast, lunch, snacks, drinks.\r\n",
      "- Ergonomic desk options from top to bottom.\r\n",
      "- Monthly fitness stipend. Yoga, CrossFit, Spin … your choice.\r\n",
      "- Monthly NYC metro card (or similar public transit pass) paid for.\r\n",
      "- You’ll receive a new MacBook Pro with appropriate software installed to complete your work.\r\n",
      "- Full medical, dental and vision benefits program. – \n"
     ]
    }
   ],
   "source": [
    "print(df.company_descr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
